[<Line: +"""Solvers for Ridge and LogisticRegression using SAG algorithm"""
>, <Line: +# Authors: Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>
>, <Line: +#
>, <Line: +# Licence: BSD 3 clause
>, <Line: +import numpy as np
>, <Line: +from ..utils import check_array
>, <Line: +from .base import make_dataset
>, <Line: +from .sgd_fast import Log, SquaredLoss
>, <Line: +from .sag_fast import sag, get_max_squared_sum
>, <Line: +def get_auto_step_size(max_squared_sum, alpha_scaled, loss, fit_intercept):
>, <Line: +    """Compute automatic step size for SAG solver
>, <Line: +    The step size is set to 1 / (alpha_scaled + L + fit_intercept) where L is
>, <Line: +    the max sum of squares for over all samples.
>, <Line: +    max_squared_sum : float
>, <Line: +        Maximum squared sum of X over samples.
>, <Line: +    alpha_scaled : float
>, <Line: +        Constant that multiplies the regularization term, scaled by
>, <Line: +        1. / n_samples, the number of samples.
>, <Line: +    loss : string, in {"log", "squared"}
>, <Line: +        The loss function used in SAG solver.
>, <Line: +    fit_intercept : bool
>, <Line: +        Specifies if a constant (a.k.a. bias or intercept) will be
>, <Line: +        added to the decision function.
>, <Line: +    Returns
>, <Line: +    -------
>, <Line: +    step_size : float
>, <Line: +        Step size used in SAG solver.
>, <Line: +    if loss == 'log':
>, <Line: +        # inverse Lipschitz constant for log loss
>, <Line: +        return 4.0 / (max_squared_sum + int(fit_intercept)
>, <Line: +                      + 4.0 * alpha_scaled)
>, <Line: +    elif loss == 'squared':
>, <Line: +        # inverse Lipschitz constant for squared loss
>, <Line: +        return 1.0 / (max_squared_sum + int(fit_intercept) + alpha_scaled)
>, <Line: +    else:
>, <Line: +        raise ValueError("Unknown loss function for SAG solver, got %s "
>, <Line: +                         "instead of 'log' or 'squared'" % loss)
>, <Line: +def sag_solver(X, y, sample_weight=None, loss='log', alpha=1.,
>, <Line: +               max_iter=1000, tol=0.001, verbose=0, random_state=None,
>, <Line: +               check_input=True, max_squared_sum=None,
>, <Line: +               warm_start_mem=dict()):
>, <Line: +    """SAG solver for Ridge and LogisticRegression
>, <Line: +    a constant learning rate.
>, <Line: +    IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
>, <Line: +    same scale. You can normalize the data by using
>, <Line: +    sklearn.preprocessing.StandardScaler on your data before passing it to the
>, <Line: +    fit method.
>, <Line: +    This implementation works with data represented as dense numpy arrays or
>, <Line: +    sparse scipy arrays of floating point values for the features. It will
>, <Line: +    fit the data according to squared loss or log loss.
>, <Line: +    The regularizer is a penalty added to the loss function that shrinks model
>, <Line: +    parameters towards the zero vector using the squared euclidean norm L2.
>, <Line: +    X : {array-like, sparse matrix}, shape (n_samples, n_features)
>, <Line: +        Training data
>, <Line: +    y : numpy array, shape (n_samples,)
>, <Line: +        Target values
>, <Line: +    sample_weight : array-like, shape (n_samples,), optional
>, <Line: +        Weights applied to individual samples (1. for unweighted).
>, <Line: +    loss : 'log' | 'squared'
>, <Line: +        Loss function that will be optimized.
>, <Line: +        'log' is used for classification, like in LogisticRegression.
>, <Line: +        'squared' is used for regression, like in Ridge.
>, <Line: +    alpha : float, optional
>, <Line: +        Constant that multiplies the regularization term. Defaults to 1.
>, <Line: +        The stopping criterea for the weights. The iterations will stop when
>, <Line: +    random_state : int seed, RandomState instance, or None (default)
>, <Line: +        The seed of the pseudo random number generator to use when
>, <Line: +        shuffling the data.
>, <Line: +    check_input : bool, default True
>, <Line: +        If False, the input arrays X and y will not be checked.
>, <Line: +    max_squared_sum : float, default None
>, <Line: +        Maximum squared sum of X over samples. If None, it will be computed,
>, <Line: +        going through all the samples. The value should be precomputed
>, <Line: +        to speed up cross validation.
>, <Line: +    warm_start_mem: dict, optional
>, <Line: +        The initialization parameters used for warm starting. It is currently
>, <Line: +        not used in Ridge.
>, <Line: +    Returns
>, <Line: +    -------
>, <Line: +    coef_ : array, shape (n_features)
>, <Line: +        Weight vector.
>, <Line: +    n_iter_ : int
>, <Line: +        The number of full pass on all samples.
>, <Line: +    warm_start_mem : dict
>, <Line: +        Contains a 'coef' key with the fitted result, and eventually the
>, <Line: +        fitted intercept at the end of the array. Contains also other keys
>, <Line: +        used for warm starting.
>, <Line: +    >>> y = np.random.randn(n_samples)
>, <Line: +    >>> clf = linear_model.Ridge(solver='sag')
>, <Line: +    Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
>, <Line: +          normalize=False, random_state=None, solver='sag', tol=0.001)
>, <Line: +    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
>, <Line: +    >>> y = np.array([1, 1, 2, 2])
>, <Line: +    >>> clf = linear_model.LogisticRegression(solver='sag')
>, <Line: +    >>> clf.fit(X, y)
>, <Line: +    ... #doctest: +NORMALIZE_WHITESPACE
>, <Line: +    LogisticRegression(C=1.0, class_weight=None, dual=False,
>, <Line: +        fit_intercept=True, intercept_scaling=1, max_iter=100,
>, <Line: +        multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
>, <Line: +        solver='sag', tol=0.0001, verbose=0, warm_start=False)
>, <Line: +    Reference
>, <Line: +    ---------
>, <Line: +    Schmidt, M., Roux, N. L., & Bach, F. (2013).
>, <Line: +    Minimizing finite sums with the stochastic average gradient
>, <Line: +    https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
>, <Line: +    Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and
>, <Line: +    LogisticRegression, SGDClassifier, LinearSVC, Perceptron
>, <Line: +    # Ridge default max_iter is None
>, <Line: +    if max_iter is None:
>, <Line: +        max_iter = 1000
>, <Line: +    if check_input:
>, <Line: +        X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
>, <Line: +        y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')
>, <Line: +    n_samples, n_features = X.shape[0], X.shape[1]
>, <Line: +    # As in SGD, the alpha is scaled by n_samples.
>, <Line: +    alpha_scaled = float(alpha) / n_samples
>, <Line: +    # initialization
>, <Line: +    if sample_weight is None:
>, <Line: +        sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
>, <Line: +    if 'coef' in warm_start_mem.keys():
>, <Line: +        coef_init = warm_start_mem['coef']
>, <Line: +    else:
>, <Line: +        coef_init = np.zeros(n_features, dtype=np.float64, order='C')
>, <Line: +    # coef_init contains possibly the intercept_init at the end.
>, <Line: +    # Note that Ridge centers the data before fitting, so fit_intercept=False.
>, <Line: +    fit_intercept = coef_init.size == (n_features + 1)
>, <Line: +    if fit_intercept:
>, <Line: +        intercept_init = coef_init[-1]
>, <Line: +        coef_init = coef_init[:-1]
>, <Line: +    else:
>, <Line: +        intercept_init = 0.0
>, <Line: +    if 'intercept_sum_gradient' in warm_start_mem.keys():
>, <Line: +        intercept_sum_gradient_init = warm_start_mem['intercept_sum_gradient']
>, <Line: +    else:
>, <Line: +        intercept_sum_gradient_init = 0.0
>, <Line: +    if 'gradient_memory' in warm_start_mem.keys():
>, <Line: +        gradient_memory_init = warm_start_mem['gradient_memory']
>, <Line: +    else:
>, <Line: +        gradient_memory_init = np.zeros(n_samples, dtype=np.float64,
>, <Line: +                                        order='C')
>, <Line: +    if 'sum_gradient' in warm_start_mem.keys():
>, <Line: +        sum_gradient_init = warm_start_mem['sum_gradient']
>, <Line: +    else:
>, <Line: +        sum_gradient_init = np.zeros(n_features, dtype=np.float64, order='C')
>, <Line: +    if 'seen' in warm_start_mem.keys():
>, <Line: +        seen_init = warm_start_mem['seen']
>, <Line: +    else:
>, <Line: +        seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
>, <Line: +    if 'num_seen' in warm_start_mem.keys():
>, <Line: +        num_seen_init = warm_start_mem['num_seen']
>, <Line: +    else:
>, <Line: +        num_seen_init = 0
>, <Line: +    dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)
>, <Line: +    if max_squared_sum is None:
>, <Line: +        max_squared_sum = get_max_squared_sum(X)
>, <Line: +    step_size = get_auto_step_size(max_squared_sum, alpha_scaled, loss,
>, <Line: +                                   fit_intercept)
>, <Line: +    if step_size * alpha_scaled == 1:
>, <Line: +        raise ZeroDivisionError("Current sag implementation does not handle "
>, <Line: +                                "the case step_size * alpha_scaled == 1")
>, <Line: +    if loss == 'log':
>, <Line: +        class_loss = Log()
>, <Line: +    elif loss == 'squared':
>, <Line: +        class_loss = SquaredLoss()
>, <Line: +    else:
>, <Line: +        raise ValueError("Invalid loss parameter: got %r instead of "
>, <Line: +                         "one of ('log', 'squared')" % loss)
>, <Line: +    intercept_, num_seen, n_iter_, intercept_sum_gradient = \
>, <Line: +        sag(dataset, coef_init.ravel(),
>, <Line: +            intercept_init, n_samples,
>, <Line: +            n_features, tol,
>, <Line: +            max_iter,
>, <Line: +            class_loss,
>, <Line: +            step_size, alpha_scaled,
>, <Line: +            sum_gradient_init.ravel(),
>, <Line: +            gradient_memory_init.ravel(),
>, <Line: +            seen_init.ravel(),
>, <Line: +            num_seen_init,
>, <Line: +            fit_intercept,
>, <Line: +            intercept_sum_gradient_init,
>, <Line: +            intercept_decay,
>, <Line: +            verbose)
>, <Line: +    if n_iter_ == max_iter:
>, <Line: +        warnings.warn("The max_iter was reached which means "
>, <Line: +                      "the coef_ did not converge", ConvergenceWarning)
>, <Line: +    coef_ = coef_init
>, <Line: +    if fit_intercept:
>, <Line: +        coef_ = np.append(coef_, intercept_)
>, <Line: +    warm_start_mem = {'coef': coef_, 'sum_gradient': sum_gradient_init,
>, <Line: +                      'intercept_sum_gradient': intercept_sum_gradient,
>, <Line: +                      'gradient_memory': gradient_memory_init,
>, <Line: +                      'seen': seen_init, 'num_seen': num_seen}
>, <Line: +    return coef_, n_iter_, warm_start_mem
>]
[<Line: -import numpy as np
>, <Line: -import scipy.sparse as sp
>, <Line: -from abc import ABCMeta
>, <Line: -from .base import LinearClassifierMixin, LinearModel, SparseCoefMixin
>, <Line: -from ..base import RegressorMixin, BaseEstimator
>, <Line: -from ..utils import check_X_y, compute_class_weight, check_random_state
>, <Line: -from ..utils.seq_dataset import ArrayDataset, CSRDataset
>, <Line: -from ..externals import six
>, <Line: -from ..externals.joblib import Parallel, delayed
>, <Line: -from .sag_fast import Log, SquaredLoss
>, <Line: -from .sag_fast import sag_sparse, get_auto_eta
>, <Line: -MAX_INT = np.iinfo(np.int32).max
>, <Line: -"""For sparse data intercept updates are scaled by this decay factor to avoid
>, <Line: -intercept oscillation."""
>, <Line: -SPARSE_INTERCEPT_DECAY = 0.01
>, <Line: -# taken from http://stackoverflow.com/questions/1816958
>, <Line: -# useful for passing instance methods to Parallel
>, <Line: -def multiprocess_method(instance, name, args=()):
>, <Line: -    "indirect caller for instance methods and multiprocessing"
>, <Line: -    return getattr(instance, name)(*args)
>, <Line: -# The inspiration for SAG comes from:
>, <Line: -# "Minimizing Finite Sums with the Stochastic Average Gradient" by
>, <Line: -# Mark Schmidt, Nicolas Le Roux, Francis Bach. 2013. <hal-00860051>
>, <Line: -#
>, <Line: -# https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
>, <Line: -class BaseSAG(six.with_metaclass(ABCMeta, SparseCoefMixin)):
>, <Line: -    def __init__(self, alpha=0.0001, fit_intercept=True, max_iter=1000,
>, <Line: -                 tol=0.001, verbose=0,
>, <Line: -                 random_state=None, eta0='auto', warm_start=False):
>, <Line: -        self.alpha = alpha
>, <Line: -        self.fit_intercept = fit_intercept
>, <Line: -        self.max_iter = max_iter
>, <Line: -        self.tol = tol
>, <Line: -        self.verbose = verbose
>, <Line: -        self.eta0 = eta0
>, <Line: -        self.random_state = random_state
>, <Line: -        self.warm_start = warm_start
>, <Line: -        self._validate_params()
>, <Line: -        self.coef_ = None
>, <Line: -        self.intercept_ = None
>, <Line: -        self.num_seen_ = None
>, <Line: -        self.seen_ = None
>, <Line: -        self.sum_gradient_ = None
>, <Line: -        self.gradient_memory_ = None
>, <Line: -        self.intercept_sum_gradient_ = None
>, <Line: -    def _validate_params(self):
>, <Line: -        if not isinstance(self.max_iter, int):
>, <Line: -            raise ValueError("max_iter must be an integer")
>, <Line: -        if self.max_iter < 1:
>, <Line: -            raise ValueError("max_iter must be greater than 0")
>, <Line: -    def _fit(self, X, y, coef_init=None, intercept_init=None,
>, <Line: -             sample_weight=None, sum_gradient_init=None,
>, <Line: -             gradient_memory_init=None, seen_init=None, num_seen_init=None,
>, <Line: -             intercept_sum_gradient_init=None,
>, <Line: -             weight_pos=1.0, weight_neg=1.0):
>, <Line: -        n_samples, n_features = X.shape[0], X.shape[1]
>, <Line: -        # initialize all parameters if there is no init
>, <Line: -        if sample_weight is None:
>, <Line: -            sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
>, <Line: -        if intercept_init is None:
>, <Line: -            intercept_init = 0.0
>, <Line: -        if intercept_sum_gradient_init is None:
>, <Line: -            intercept_sum_gradient_init = 0.0
>, <Line: -        if coef_init is None:
>, <Line: -            coef_init = np.zeros(n_features, dtype=np.float64, order='C')
>, <Line: -        if sum_gradient_init is None:
>, <Line: -            sum_gradient_init = np.zeros(n_features, dtype=np.float64,
>, <Line: -                                         order='C')
>, <Line: -        if gradient_memory_init is None:
>, <Line: -            gradient_memory_init = np.zeros(n_samples, dtype=np.float64,
>, <Line: -                                            order='C')
>, <Line: -        if seen_init is None:
>, <Line: -            seen_init = np.zeros(n_samples, dtype=np.int32, order='C')
>, <Line: -        if num_seen_init is None:
>, <Line: -            num_seen_init = 0
>, <Line: -        random_state = check_random_state(self.random_state)
>, <Line: -        # check which type of Sequential Dataset is needed
>, <Line: -        if sp.issparse(X):
>, <Line: -            dataset = CSRDataset(X.data, X.indptr, X.indices,
>, <Line: -                                 y, sample_weight,
>, <Line: -                                 seed=random_state.randint(MAX_INT))
>, <Line: -            intercept_decay = SPARSE_INTERCEPT_DECAY
>, <Line: -        else:
>, <Line: -            dataset = ArrayDataset(X, y, sample_weight,
>, <Line: -                                   seed=random_state.randint(MAX_INT))
>, <Line: -            intercept_decay = 1.0
>, <Line: -        # set the eta0 if needed, 'auto' is 1 / 4L where L is the max sum of
>, <Line: -        # squares for over all samples
>, <Line: -        if self.eta0 == 'auto':
>, <Line: -            step_size = get_auto_eta(dataset, self.alpha, n_samples,
>, <Line: -                                     self.loss_function, self.fit_intercept)
>, <Line: -        else:
>, <Line: -            step_size = self.eta0
>, <Line: -        intercept_, num_seen, max_iter_reached, intercept_sum_gradient = \
>, <Line: -            sag_sparse(dataset, coef_init.ravel(),
>, <Line: -                       intercept_init, n_samples,
>, <Line: -                       n_features, self.tol,
>, <Line: -                       self.max_iter,
>, <Line: -                       self.loss_function,
>, <Line: -                       step_size, self.alpha,
>, <Line: -                       sum_gradient_init.ravel(),
>, <Line: -                       gradient_memory_init.ravel(),
>, <Line: -                       seen_init.ravel(),
>, <Line: -                       num_seen_init, weight_pos,
>, <Line: -                       weight_neg,
>, <Line: -                       self.fit_intercept,
>, <Line: -                       intercept_sum_gradient_init,
>, <Line: -                       intercept_decay,
>, <Line: -                       self.verbose)
>, <Line: -        if max_iter_reached:
>, <Line: -            warnings.warn("The max_iter was reached which means "
>, <Line: -                          "the coef_ did not converge", ConvergenceWarning)
>, <Line: -        return (coef_init.reshape(1, -1), intercept_,
>, <Line: -                sum_gradient_init.reshape(1, -1),
>, <Line: -                gradient_memory_init.reshape(1, -1),
>, <Line: -                seen_init.reshape(1, -1),
>, <Line: -                num_seen, intercept_sum_gradient)
>, <Line: -class SAGClassifier(BaseSAG, LinearClassifierMixin, BaseEstimator):
>, <Line: -    """Linear classifiers (SVM, logistic regression, a.o.) with SAG training.
>, <Line: -    This estimator implements regularized linear models with stochastic
>, <Line: -    average gradient (SAG) learning: the gradient of the loss is estimated
>, <Line: -    using a random sample from the dataset. The weights are then updated
>, <Line: -    according to the sum of gradients seen thus far divided by the number of
>, <Line: -    unique samples seen. The inspiration for SAG comes from "Minimizing Finite
>, <Line: -    Sums with the Stochastic Average Gradient" by Mark Schmidt, Nicolas Le
>, <Line: -    Roux, and Francis Bach. 2013. <hal-00860051>
>, <Line: -    https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
>, <Line: -    IMPORTANT NOTE: SAGClassifier and models from linear_model in general
>, <Line: -    depend on columns that are on the same scale. You can make sure that the
>, <Line: -    data will be normalized by using sklearn.preprocessing.StandardScaler on
>, <Line: -    your data before passing it to the fit method.
>, <Line: -    This implementation works with data represented as dense or sparse arrays
>, <Line: -    of floating point values for the features. It will fit the data according
>, <Line: -    to log loss.
>, <Line: -    The regularizer is a penalty added to the loss function that shrinks model
>, <Line: -    parameters towards the zero vector using either the squared euclidean norm
>, <Line: -    L2.
>, <Line: -    alpha : float, optional
>, <Line: -        Constant that multiplies the regularization term. Defaults to 0.0001
>, <Line: -    fit_intercept: bool, optional
>, <Line: -        Whether the intercept should be estimated or not. If False, the
>, <Line: -        data is assumed to be already centered. Defaults to True.
>, <Line: -    max_iter: int, optional
>, <Line: -        The max number of passes over the training data if the stopping
>, <Line: -        criterea is not reached. Defaults to 1000.
>, <Line: -    tol: double, optional
>, <Line: -        The stopping criterea for the weights. THe iterations will stop when
>, <Line: -        max(change in weights) / max(weights) < tol. Defaults to .001
>, <Line: -    random_state: int or numpy.random.RandomState, optional
>, <Line: -        The random_state of the pseudo random number generator to use when
>, <Line: -        sampling the data.
>, <Line: -    verbose: integer, optional
>, <Line: -        The verbosity level
>, <Line: -    n_jobs: integer, optional
>, <Line: -        The number of CPUs to use to do the OVA (One Versus All, for
>, <Line: -        multi-class problems) computation. -1 means 'all CPUs'. Defaults
>, <Line: -        to 1.
>, <Line: -    eta0 : double or "auto"
>, <Line: -        The initial learning rate. The default value is 0.001.
>, <Line: -    class_weight : dict, {class_label : weight} or "auto" or None, optional
>, <Line: -        Preset for the class_weight fit parameter.
>, <Line: -        Weights associated with classes. If not given, all classes
>, <Line: -        are supposed to have weight one.
>, <Line: -        The "auto" mode uses the values of y to automatically adjust
>, <Line: -        weights inversely proportional to class frequencies.
>, <Line: -    warm_start : bool, optional
>, <Line: -        When set to True, reuse the solution of the previous call to fit as
>, <Line: -        initialization, otherwise, just erase the previous solution.
>, <Line: -    Attributes
>, <Line: -    ----------
>, <Line: -    coef_ : array, shape (1, n_features) if n_classes == 2 else (n_classes,
>, <Line: -    n_features)
>, <Line: -        Weights assigned to the features.
>, <Line: -    intercept_ : array, shape (1,) if n_classes == 2 else (n_classes,)
>, <Line: -        Constants in decision function.
>, <Line: -    Examples
>, <Line: -    --------
>, <Line: -    >>> import numpy as np
>, <Line: -    >>> from sklearn import linear_model
>, <Line: -    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
>, <Line: -    >>> Y = np.array([1, 1, 2, 2])
>, <Line: -    >>> clf = linear_model.SAGClassifier()
>, <Line: -    >>> clf.fit(X, Y)
>, <Line: -    ... #doctest: +NORMALIZE_WHITESPACE
>, <Line: -    SAGClassifier(alpha=0.0001, class_weight=None,
>, <Line: -                  eta0='auto', fit_intercept=True,
>, <Line: -                  max_iter=1000, n_jobs=1, random_state=None,
>, <Line: -                  tol=0.001, verbose=0, warm_start=False)
>, <Line: -    >>> print(clf.predict([[-0.8, -1]]))
>, <Line: -    [1]
>, <Line: -    See also
>, <Line: -    --------
>, <Line: -    SGDClassifier, LinearSVC, LogisticRegression, Perceptron
>, <Line: -    def __init__(self, alpha=0.0001,
>, <Line: -                 fit_intercept=True, max_iter=1000, tol=0.001, verbose=0,
>, <Line: -                 n_jobs=1, random_state=None,
>, <Line: -                 eta0='auto', class_weight=None, warm_start=False):
>, <Line: -        self.n_jobs = n_jobs
>, <Line: -        self.class_weight = class_weight
>, <Line: -        self.loss_function = Log()
>, <Line: -        super(SAGClassifier, self).__init__(alpha=alpha,
>, <Line: -                                            fit_intercept=fit_intercept,
>, <Line: -                                            max_iter=max_iter,
>, <Line: -                                            verbose=verbose,
>, <Line: -                                            random_state=random_state,
>, <Line: -                                            tol=tol,
>, <Line: -                                            eta0=eta0,
>, <Line: -                                            warm_start=warm_start)
>, <Line: -    """Fit linear model with Stochastic Average Gradient.
>, <Line: -        Parameters
>, <Line: -        ----------
>, <Line: -        X : {array-like, sparse matrix}, shape (n_samples, n_features)
>, <Line: -            Training data
>, <Line: -        y : numpy array, shape (n_samples,)
>, <Line: -            Target values
>, <Line: -        sample_weight : array-like, shape (n_samples,), optional
>, <Line: -            Weights applied to individual samples (1. for unweighted).
>, <Line: -        Returns
>, <Line: -        -------
>, <Line: -        self : returns an instance of self.
>, <Line: -        """
>, <Line: -    def fit(self, X, y, sample_weight=None):
>, <Line: -        X, y = check_X_y(X, y, "csr", copy=False, order='C',
>, <Line: -                         dtype=np.float64)
>, <Line: -        n_samples, n_features = X.shape[0], X.shape[1]
>, <Line: -        self.classes_ = np.unique(y)
>, <Line: -        self.expanded_class_weight_ = compute_class_weight(self.class_weight,
>, <Line: -                                                           self.classes_, y)
>, <Line: -        if self.classes_.shape[0] <= 1:
>, <Line: -            # there is only one class
>, <Line: -            raise ValueError("The number of class labels must be "
>, <Line: -                             "greater than one.")
>, <Line: -        elif self.classes_.shape[0] == 2:
>, <Line: -            # binary classifier
>, <Line: -            (coef, intercept, sum_gradient, gradient_memory,
>, <Line: -             seen, num_seen, intercept_sum_gradient) = \
>, <Line: -                self._fit_target_class(X, y, self.classes_[1], sample_weight)
>, <Line: -        else:
>, <Line: -            # multiclass classifier
>, <Line: -            coef = []
>, <Line: -            intercept = []
>, <Line: -            sum_gradient = []
>, <Line: -            gradient_memory = []
>, <Line: -            seen = []
>, <Line: -            num_seen = []
>, <Line: -            intercept_sum_gradient = []
>, <Line: -            # perform a fit for all classes, one verse all
>, <Line: -            results = Parallel(n_jobs=self.n_jobs,
>, <Line: -                               backend="threading",
>, <Line: -                               verbose=self.verbose)(
>, <Line: -                # we have to use a call to multiprocess_method instead of the
>, <Line: -                # plain instance method because pickle will not work on
>, <Line: -                # instance methods in python 2.6 and 2.7
>, <Line: -                delayed(multiprocess_method)(self, "_fit_target_class",
>, <Line: -                                             (X, y, cl, sample_weight))
>, <Line: -                for cl in self.classes_)
>, <Line: -            # append results to the correct array
>, <Line: -            for (coef_cl, intercept_cl, sum_gradient_cl, gradient_memory_cl,
>, <Line: -                 seen_cl, num_seen_cl, intercept_sum_gradient_cl) in results:
>, <Line: -                coef.append(coef_cl)
>, <Line: -                intercept.append(intercept_cl)
>, <Line: -                sum_gradient.append(sum_gradient_cl)
>, <Line: -                gradient_memory.append(gradient_memory_cl)
>, <Line: -                seen.append(seen_cl)
>, <Line: -                num_seen.append(num_seen_cl)
>, <Line: -                intercept_sum_gradient.append(intercept_sum_gradient_cl)
>, <Line: -            # stack all arrays to transform into np arrays
>, <Line: -            coef = np.vstack(coef)
>, <Line: -            intercept = np.array(intercept)
>, <Line: -            sum_gradient = np.vstack(sum_gradient)
>, <Line: -            gradient_memory = np.vstack(gradient_memory)
>, <Line: -            seen = np.vstack(seen)
>, <Line: -            num_seen = np.array(num_seen)
>, <Line: -            intercept_sum_gradient = np.array(intercept_sum_gradient)
>, <Line: -        self.coef_ = coef
>, <Line: -        self.intercept_ = intercept
>, <Line: -        self.sum_gradient_ = sum_gradient
>, <Line: -        self.gradient_memory_ = gradient_memory
>, <Line: -        self.seen_ = seen
>, <Line: -        self.num_seen_ = num_seen
>, <Line: -        self.intercept_sum_gradient_ = intercept_sum_gradient
>, <Line: -        return self
>, <Line: -    def _fit_target_class(self, X, y, target_class, sample_weight=None):
>, <Line: -        coef_init = None
>, <Line: -        intercept_init = None
>, <Line: -        sum_gradient_init = None
>, <Line: -        gradient_memory_init = None
>, <Line: -        seen_init = None
>, <Line: -        num_seen_init = None
>, <Line: -        intercept_sum_gradient_init = None
>, <Line: -        if self.classes_.shape[0] == 2:
>, <Line: -            if self.warm_start:
>, <Line: -                # init parameters for binary classifier
>, <Line: -                coef_init = self.coef_
>, <Line: -                intercept_init = self.intercept_
>, <Line: -                sum_gradient_init = self.sum_gradient_
>, <Line: -                gradient_memory_init = self.gradient_memory_
>, <Line: -                seen_init = self.seen_
>, <Line: -                num_seen_init = self.num_seen_
>, <Line: -                intercept_sum_gradient_init = \
>, <Line: -                    self.intercept_sum_gradient_
>, <Line: -            weight_pos = self.expanded_class_weight_[1]
>, <Line: -            weight_neg = self.expanded_class_weight_[0]
>, <Line: -        else:
>, <Line: -            class_index = np.where(self.classes_ == target_class)[0][0]
>, <Line: -            if self.warm_start:
>, <Line: -                # init parameters for multi-class classifier
>, <Line: -                if self.coef_ is not None:
>, <Line: -                    coef_init = self.coef_[class_index]
>, <Line: -                if self.intercept_ is not None:
>, <Line: -                    intercept_init = self.intercept_[class_index]
>, <Line: -                if self.sum_gradient_ is not None:
>, <Line: -                    sum_gradient_init = self.sum_gradient_[class_index]
>, <Line: -                if self.gradient_memory_ is not None:
>, <Line: -                    gradient_memory_init = self.gradient_memory_[class_index]
>, <Line: -                if self.seen_ is not None:
>, <Line: -                    seen_init = self.seen_[class_index]
>, <Line: -                if self.num_seen_ is not None:
>, <Line: -                    num_seen_init = self.num_seen_[class_index]
>, <Line: -                if self.intercept_sum_gradient_ is not None:
>, <Line: -                    intercept_sum_gradient_init = \
>, <Line: -                        self.intercept_sum_gradient_[class_index]
>, <Line: -            weight_pos = self.expanded_class_weight_[class_index]
>, <Line: -            weight_neg = 1.0
>, <Line: -        n_samples, n_features = X.shape[0], X.shape[1]
>, <Line: -        y_encoded = np.ones(n_samples)
>, <Line: -        y_encoded[y != target_class] = -1.0
>, <Line: -        return super(SAGClassifier, self).\
>, <Line: -            _fit(X, y_encoded,
>, <Line: -                 coef_init, intercept_init,
>, <Line: -                 sample_weight,
>, <Line: -                 sum_gradient_init,
>, <Line: -                 gradient_memory_init,
>, <Line: -                 seen_init, num_seen_init,
>, <Line: -                 intercept_sum_gradient_init,
>, <Line: -                 weight_pos, weight_neg)
>, <Line: -class SAGRegressor(BaseSAG, LinearModel, RegressorMixin,
>, <Line: -                   BaseEstimator):
>, <Line: -    """Linear model fitted by minimizing a regularized empirical loss with SAG
>, <Line: -    a constant learning rate. The inspiration for SAG comes from "Minimizing
>, <Line: -    Finite Sums with the Stochastic Average Gradient" by Mark Schmidt,
>, <Line: -    Nicolas Le Roux, and Francis Bach. 2013. <hal-00860051>
>, <Line: -    https://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf
>, <Line: -    IMPORTANT NOTE: SAGRegressor and models from linear_model in general depend
>, <Line: -    on columns that are on the same scale. You can make sure that the data will
>, <Line: -    be normalized by using sklearn.preprocessing.StandardScaler on your data
>, <Line: -    before passing it to the fit method.
>, <Line: -    The regularizer is a penalty added to the loss function that shrinks model
>, <Line: -    parameters towards the zero vector using the squared euclidean norm
>, <Line: -    L2.
>, <Line: -    This implementation works with data represented as dense or sparse numpy
>, <Line: -    arrays of floating point values for the features.
>, <Line: -    alpha : float, optional
>, <Line: -        Constant that multiplies the regularization term. Defaults to 0.0001
>, <Line: -    fit_intercept: bool, optional
>, <Line: -        Whether the intercept should be estimated or not. If False, the
>, <Line: -        data is assumed to be already centered. Defaults to True.
>, <Line: -        The stopping criterea for the weights. THe iterations will stop when
>, <Line: -    random_state: int or numpy.random.RandomState, optional
>, <Line: -        The random_state of the pseudo random number generator to use when
>, <Line: -        sampling the data.
>, <Line: -    eta0 : double or "auto"
>, <Line: -        The initial learning rate [default 0.01].
>, <Line: -    warm_start : bool, optional
>, <Line: -        When set to True, reuse the solution of the previous call to fit as
>, <Line: -        initialization, otherwise, just erase the previous solution.
>, <Line: -    Attributes
>, <Line: -    ----------
>, <Line: -    coef_ : array, shape (n_features,)
>, <Line: -        Weights asigned to the features.
>, <Line: -    intercept_ : array, shape (1,)
>, <Line: -        The intercept term.
>, <Line: -    >>> y = np.random.randn(n_samples)
>, <Line: -    >>> clf = linear_model.SAGRegressor()
>, <Line: -    SAGRegressor(alpha=0.0001, eta0='auto',
>, <Line: -                 fit_intercept=True, max_iter=1000, random_state=None,
>, <Line: -                 tol=0.001, verbose=0, warm_start=False)
>, <Line: -    SGDRegressor, Ridge, ElasticNet, Lasso, SVR
>, <Line: -    def __init__(self, alpha=0.0001, fit_intercept=True, max_iter=1000,
>, <Line: -                 tol=0.001, verbose=0, random_state=None, eta0='auto',
>, <Line: -                 warm_start=False):
>, <Line: -        self.loss_function = SquaredLoss()
>, <Line: -        super(SAGRegressor, self).__init__(alpha=alpha,
>, <Line: -                                           fit_intercept=fit_intercept,
>, <Line: -                                           max_iter=max_iter,
>, <Line: -                                           verbose=verbose,
>, <Line: -                                           random_state=random_state,
>, <Line: -                                           tol=tol,
>, <Line: -                                           eta0=eta0,
>, <Line: -                                           warm_start=warm_start)
>, <Line: -    """Fit linear model with Stochastic Average Gradient.
>, <Line: -        Parameters
>, <Line: -        ----------
>, <Line: -        X : {array-like, sparse matrix}, shape (n_samples, n_features)
>, <Line: -            Training data
>, <Line: -        y : numpy array, shape (n_samples,)
>, <Line: -            Target values
>, <Line: -        sample_weight : array-like, shape (n_samples,), optional
>, <Line: -            Weights applied to individual samples (1. for unweighted).
>, <Line: -        Returns
>, <Line: -        -------
>, <Line: -        self : returns an instance of self.
>, <Line: -        """
>, <Line: -    def fit(self, X, y, sample_weight=None):
>, <Line: -        X, y = check_X_y(X, y, "csr", copy=False, order='C', dtype=np.float64)
>, <Line: -        y = y.astype(np.float64)
>, <Line: -        coef_init = None
>, <Line: -        intercept_init = None
>, <Line: -        sum_gradient_init = None
>, <Line: -        gradient_memory_init = None
>, <Line: -        seen_init = None
>, <Line: -        num_seen_init = None
>, <Line: -        intercept_sum_gradient_init = None
>, <Line: -        if self.warm_start:
>, <Line: -            coef_init = self.coef_
>, <Line: -            intercept_init = self.intercept_
>, <Line: -            sum_gradient_init = self.sum_gradient_
>, <Line: -            gradient_memory_init = self.gradient_memory_
>, <Line: -            seen_init = self.seen_
>, <Line: -            num_seen_init = self.num_seen_
>, <Line: -            intercept_sum_gradient_init = self.intercept_sum_gradient_
>, <Line: -        (self.coef_, self.intercept_, self.sum_gradient_,
>, <Line: -         self.gradient_memory_, self.seen_, self.num_seen_,
>, <Line: -         self.intercept_sum_gradient_) = \
>, <Line: -            super(SAGRegressor, self)._fit(X, y, coef_init,
>, <Line: -                                           intercept_init,
>, <Line: -                                           sample_weight,
>, <Line: -                                           sum_gradient_init,
>, <Line: -                                           gradient_memory_init,
>, <Line: -                                           seen_init, num_seen_init,
>, <Line: -                                           intercept_sum_gradient_init)
>, <Line: -        return self
>]