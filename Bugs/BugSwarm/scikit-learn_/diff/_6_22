[<Line: +# Authors: Danny Sullivan <dbsullivan23@gmail.com>
>, <Line: +#          Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>
>, <Line: +#
>, <Line: +# Licence: BSD 3 clause
>, <Line: +import math
>, <Line: +from sklearn.linear_model.sag import get_auto_step_size
>, <Line: +from sklearn.linear_model.sag_fast import get_max_squared_sum
>, <Line: +from sklearn.utils.testing import assert_raise_message
>, <Line: +from sklearn.utils.testing import ignore_warnings
>, <Line: +from sklearn.utils import compute_class_weight
>, <Line: +from sklearn.preprocessing import LabelEncoder
>, <Line: +from sklearn.base import clone
>, <Line: +def sag(X, y, step_size, alpha, n_iter=1, dloss=None, sparse=False,
>, <Line: +        sample_weight=None, fit_intercept=True):
>, <Line: +            if sample_weight is not None:
>, <Line: +                gradient *= sample_weight[idx]
>, <Line: +                intercept -= (step_size * intercept_sum_gradient
>, <Line: +                              / len(seen) * decay)
>, <Line: +            weights -= step_size * sum_gradient / len(seen)
>, <Line: +def sag_sparse(X, y, step_size, alpha, n_iter=1,
>, <Line: +               dloss=None, sample_weight=None, sparse=False,
>, <Line: +    if step_size * alpha == 1.:
>, <Line: +        raise ZeroDivisionError("Sparse sag does not handle the case "
>, <Line: +                                "step_size * alpha == 1")
>, <Line: +            if sample_weight is not None:
>, <Line: +                gradient *= sample_weight[idx]
>, <Line: +                intercept -= (step_size * intercept_sum_gradient
>, <Line: +                              / len(seen) * decay)
>, <Line: +            wscale *= (1.0 - alpha * step_size)
>, <Line: +                c_sum[0] = step_size / (wscale * len(seen))
>, <Line: +                                  step_size / (wscale * len(seen)))
>, <Line: +def get_step_size(X, alpha, fit_intercept, classification=True):
>, <Line: +@ignore_warnings
>, <Line: +    n_samples = 20
>, <Line: +    n_iter = 80
>, <Line: +    step_size = get_step_size(X, alpha, fit_intercept)
>, <Line: +    clf = LogisticRegression(solver="sag", fit_intercept=fit_intercept,
>, <Line: +                             tol=1e-11, C=1. / alpha / n_samples,
>, <Line: +                             max_iter=n_iter, random_state=10)
>, <Line: +    weights, intercept = sag_sparse(X, y, step_size, alpha, n_iter=n_iter,
>, <Line: +    weights2, intercept2 = sag(X, y, step_size, alpha, n_iter=n_iter,
>, <Line: +@ignore_warnings
>, <Line: +    n_samples = 10
>, <Line: +    n_features = 5
>, <Line: +    rng = np.random.RandomState(10)
>, <Line: +    X = rng.normal(size=(n_samples, n_features))
>, <Line: +    true_w = rng.normal(size=n_features)
>, <Line: +    alpha = 1.
>, <Line: +    step_size = get_step_size(X, alpha, fit_intercept, classification=False)
>, <Line: +    clf = Ridge(fit_intercept=fit_intercept, tol=.00000000001, solver='sag',
>, <Line: +                alpha=alpha * n_samples, max_iter=n_iter)
>, <Line: +    weights1, intercept1 = sag_sparse(X, y, step_size, alpha, n_iter=n_iter,
>, <Line: +                                      dloss=squared_dloss,
>, <Line: +                                      fit_intercept=fit_intercept)
>, <Line: +    weights2, intercept2 = sag(X, y, step_size, alpha, n_iter=n_iter,
>, <Line: +    assert_array_almost_equal(weights1, clf.coef_, decimal=10)
>, <Line: +    assert_array_almost_equal(intercept1, clf.intercept_, decimal=10)
>, <Line: +@ignore_warnings
>, <Line: +    n_samples = 100
>, <Line: +    max_iter = 20
>, <Line: +    clf1 = LogisticRegression(solver='sag', fit_intercept=False, tol=.0000001,
>, <Line: +                              C=1. / alpha / n_samples, max_iter=max_iter,
>, <Line: +                              random_state=10)
>, <Line: +    clf2 = clone(clf1)
>, <Line: +                              C=1. / alpha / n_samples, max_iter=max_iter,
>, <Line: +    clf2.fit(sp.csr_matrix(X), y)
>, <Line: +@ignore_warnings
>, <Line: +    n_samples = 100
>, <Line: +    fit_intercept = False
>, <Line: +    rng = np.random.RandomState(10)
>, <Line: +    X = rng.normal(size=(n_samples, n_features))
>, <Line: +    true_w = rng.normal(size=n_features)
>, <Line: +    clf1 = Ridge(fit_intercept=fit_intercept, tol=.00000000001, solver='sag',
>, <Line: +                 alpha=alpha, max_iter=n_iter, random_state=42)
>, <Line: +    clf2 = clone(clf1)
>, <Line: +    clf3 = Ridge(fit_intercept=fit_intercept, tol=.00001, solver='lsqr',
>, <Line: +                 alpha=alpha, max_iter=n_iter, random_state=42)
>, <Line: +    clf2.fit(sp.csr_matrix(X), y)
>, <Line: +    pobj1 = get_pobj(clf1.coef_, alpha, X, y, squared_loss)
>, <Line: +    pobj2 = get_pobj(clf2.coef_, alpha, X, y, squared_loss)
>, <Line: +    pobj3 = get_pobj(clf3.coef_, alpha, X, y, squared_loss)
>, <Line: +    assert_array_almost_equal(pobj1, pobj3, decimal=4)
>, <Line: +    assert_array_almost_equal(pobj3, pobj2, decimal=4)
>, <Line: +@ignore_warnings
>, <Line: +    """tests if the sag regressor is computed correctly"""
>, <Line: +    n_features = 10
>, <Line: +    n_samples = 40
>, <Line: +    max_iter = 50
>, <Line: +    tol = .000001
>, <Line: +    fit_intercept = True
>, <Line: +    y = np.dot(X, w) + 2.
>, <Line: +    step_size = get_step_size(X, alpha, fit_intercept, classification=False)
>, <Line: +    clf1 = Ridge(fit_intercept=fit_intercept, tol=tol, solver='sag',
>, <Line: +                 alpha=alpha * n_samples, max_iter=max_iter)
>, <Line: +    clf2 = clone(clf1)
>, <Line: +    clf2.fit(sp.csr_matrix(X), y)
>, <Line: +    spweights1, spintercept1 = sag_sparse(X, y, step_size, alpha,
>, <Line: +                                          n_iter=max_iter,
>, <Line: +                                          dloss=squared_dloss,
>, <Line: +                                          fit_intercept=fit_intercept)
>, <Line: +    spweights2, spintercept2 = sag_sparse(X, y, step_size, alpha,
>, <Line: +                                          n_iter=max_iter,
>, <Line: +                                          dloss=squared_dloss, sparse=True,
>, <Line: +                                          fit_intercept=fit_intercept)
>, <Line: +                              spweights1.ravel(),
>, <Line: +                              decimal=3)
>, <Line: +    assert_almost_equal(clf1.intercept_, spintercept1, decimal=1)
>, <Line: +    # TODO: uncomment when sparse Ridge with intercept will be fixed (#4710)
>, <Line: +    #assert_array_almost_equal(clf2.coef_.ravel(),
>, <Line: +    #                          spweights2.ravel(),
>, <Line: +    #                          decimal=3)
>, <Line: +    #assert_almost_equal(clf2.intercept_, spintercept2, decimal=1)'''
>, <Line: +@ignore_warnings
>, <Line: +def test_get_auto_step_size():
>, <Line: +    X = np.array([[1, 2, 3], [2, 3, 4], [2, 3, 2]], dtype=np.float64)
>, <Line: +    alpha = 1.2
>, <Line: +    fit_intercept = False
>, <Line: +    # sum the squares of the second sample because that's the largest
>, <Line: +    max_squared_sum = 4 + 9 + 16
>, <Line: +    max_squared_sum_ = get_max_squared_sum(X)
>, <Line: +    assert_almost_equal(max_squared_sum, max_squared_sum_, decimal=4)
>, <Line: +    for fit_intercept in (True, False):
>, <Line: +        step_size_sqr = 1.0 / (max_squared_sum + alpha + int(fit_intercept))
>, <Line: +        step_size_log = 4.0 / (max_squared_sum + 4.0 * alpha +
>, <Line: +                               int(fit_intercept))
>, <Line: +        step_size_sqr_ = get_auto_step_size(max_squared_sum_, alpha, "squared",
>, <Line: +                                            fit_intercept)
>, <Line: +        step_size_log_ = get_auto_step_size(max_squared_sum_, alpha, "log",
>, <Line: +                                            fit_intercept)
>, <Line: +        assert_almost_equal(step_size_sqr, step_size_sqr_, decimal=4)
>, <Line: +        assert_almost_equal(step_size_log, step_size_log_, decimal=4)
>, <Line: +    msg = 'Unknown loss function for SAG solver, got wrong instead of'
>, <Line: +    assert_raise_message(ValueError, msg, get_auto_step_size,
>, <Line: +                         max_squared_sum_, alpha, "wrong", fit_intercept)
>, <Line: +def test_get_max_squared_sum():
>, <Line: +    n_samples = 100
>, <Line: +    n_features = 10
>, <Line: +    rng = np.random.RandomState(42)
>, <Line: +    X = rng.randn(n_samples, n_features).astype(np.float64)
>, <Line: +    mask = rng.randn(n_samples, n_features)
>, <Line: +    X[mask > 0] = 0.
>, <Line: +    X_csr = sp.csr_matrix(X)
>, <Line: +    X[0, 3] = 0.
>, <Line: +    X_csr[0, 3] = 0.
>, <Line: +    sum_X = get_max_squared_sum(X)
>, <Line: +    sum_X_csr = get_max_squared_sum(X_csr)
>, <Line: +    assert_almost_equal(sum_X, sum_X_csr)
>, <Line: +@ignore_warnings
>, <Line: +    """tests if the sag regressor performs well"""
>, <Line: +    n_samples = 20
>, <Line: +    alpha = 0.1
>, <Line: +    clf1 = Ridge(tol=tol, solver='sag', max_iter=max_iter,
>, <Line: +                 alpha=alpha * n_samples)
>, <Line: +    clf2 = clone(clf1)
>, <Line: +    clf1.fit(X, y)
>, <Line: +    clf2.fit(sp.csr_matrix(X), y)
>, <Line: +    score1 = clf1.score(X, y)
>, <Line: +    assert_greater(score1, 0.99)
>, <Line: +    clf1 = Ridge(tol=tol, solver='sag', max_iter=max_iter,
>, <Line: +                 alpha=alpha * n_samples)
>, <Line: +    clf2 = clone(clf1)
>, <Line: +    clf1.fit(X, y)
>, <Line: +    clf2.fit(sp.csr_matrix(X), y)
>, <Line: +    score1 = clf1.score(X, y)
>, <Line: +    assert_greater(score1, 0.5)
>, <Line: +@ignore_warnings
>, <Line: +    """tests if the binary classifier is computed correctly"""
>, <Line: +    n_iter = 50
>, <Line: +    tol = .00001
>, <Line: +    fit_intercept = True
>, <Line: +    step_size = get_step_size(X, alpha, fit_intercept, classification=True)
>, <Line: +    clf1 = LogisticRegression(solver='sag', C=1. / alpha / n_samples,
>, <Line: +                              max_iter=n_iter, tol=tol, random_state=77,
>, <Line: +                              fit_intercept=fit_intercept)
>, <Line: +    clf2 = clone(clf1)
>, <Line: +    clf2.fit(sp.csr_matrix(X), y)
>, <Line: +    spweights, spintercept = sag_sparse(X, y, step_size, alpha, n_iter=n_iter,
>, <Line: +                                        dloss=log_dloss,
>, <Line: +                                        fit_intercept=fit_intercept)
>, <Line: +    spweights2, spintercept2 = sag_sparse(X, y, step_size, alpha,
>, <Line: +                                          dloss=log_dloss, sparse=True,
>, <Line: +                                          fit_intercept=fit_intercept)
>, <Line: +                              decimal=2)
>, <Line: +                              decimal=2)
>, <Line: +@ignore_warnings
>, <Line: +    """tests if the multiclass classifier is computed correctly"""
>, <Line: +    n_samples = 20
>, <Line: +    tol = .00001
>, <Line: +    max_iter = 40
>, <Line: +    fit_intercept = True
>, <Line: +    step_size = get_step_size(X, alpha, fit_intercept, classification=True)
>, <Line: +    clf1 = LogisticRegression(solver='sag', C=1. / alpha / n_samples,
>, <Line: +                              max_iter=max_iter, tol=tol, random_state=77,
>, <Line: +                              fit_intercept=fit_intercept)
>, <Line: +    clf2 = clone(clf1)
>, <Line: +    clf2.fit(sp.csr_matrix(X), y)
>, <Line: +    coef1 = []
>, <Line: +    intercept1 = []
>, <Line: +    for cl in classes:
>, <Line: +        spweights1, spintercept1 = sag_sparse(X, y_encoded, step_size, alpha,
>, <Line: +                                              dloss=log_dloss, n_iter=max_iter,
>, <Line: +                                              fit_intercept=fit_intercept)
>, <Line: +        spweights2, spintercept2 = sag_sparse(X, y_encoded, step_size, alpha,
>, <Line: +                                              dloss=log_dloss, n_iter=max_iter,
>, <Line: +                                              sparse=True,
>, <Line: +                                              fit_intercept=fit_intercept)
>, <Line: +        coef1.append(spweights1)
>, <Line: +        intercept1.append(spintercept1)
>, <Line: +    coef1 = np.vstack(coef1)
>, <Line: +    intercept1 = np.array(intercept1)
>, <Line: +                                  coef1[i].ravel(),
>, <Line: +        assert_almost_equal(clf1.intercept_[i], intercept1[i], decimal=1)
>, <Line: +@ignore_warnings
>, <Line: +    """tests if classifier results match target"""
>, <Line: +    max_iter = 200
>, <Line: +    clf1 = LogisticRegression(solver='sag', C=1. / alpha / n_samples,
>, <Line: +                              max_iter=max_iter, tol=tol, random_state=77)
>, <Line: +    clf2 = clone(clf1)
>, <Line: +    clf2.fit(sp.csr_matrix(X), y)
>, <Line: +    pred1 = clf1.predict(X)
>, <Line: +    pred2 = clf2.predict(X)
>, <Line: +    assert_almost_equal(pred1, y, decimal=12)
>, <Line: +    assert_almost_equal(pred2, y, decimal=12)
>, <Line: +@ignore_warnings
>, <Line: +    n_iter = 20
>, <Line: +    tol = .00001
>, <Line: +    fit_intercept = True
>, <Line: +    X, y = make_blobs(n_samples=n_samples, centers=2, random_state=10,
>, <Line: +    step_size = get_step_size(X, alpha, fit_intercept, classification=True)
>, <Line: +    clf1 = LogisticRegression(solver='sag', C=1. / alpha / n_samples,
>, <Line: +                              max_iter=n_iter, tol=tol, random_state=77,
>, <Line: +                              fit_intercept=fit_intercept,
>, <Line: +                              class_weight=class_weight)
>, <Line: +    clf2 = clone(clf1)
>, <Line: +    clf2.fit(sp.csr_matrix(X), y)
>, <Line: +    le = LabelEncoder()
>, <Line: +    class_weight_ = compute_class_weight(class_weight, np.unique(y), y)
>, <Line: +    sample_weight = class_weight_[le.fit_transform(y)]
>, <Line: +    spweights, spintercept = sag_sparse(X, y, step_size, alpha, n_iter=n_iter,
>, <Line: +                                        sample_weight=sample_weight,
>, <Line: +                                        fit_intercept=fit_intercept)
>, <Line: +    spweights2, spintercept2 = sag_sparse(X, y, step_size, alpha,
>, <Line: +                                          dloss=log_dloss, sparse=True,
>, <Line: +                                          sample_weight=sample_weight,
>, <Line: +                                          fit_intercept=fit_intercept)
>, <Line: +@ignore_warnings
>, <Line: +    tol = .00001
>, <Line: +    max_iter = 50
>, <Line: +    fit_intercept = True
>, <Line: +    step_size = get_step_size(X, alpha, fit_intercept, classification=True)
>, <Line: +    clf1 = LogisticRegression(solver='sag', C=1. / alpha / n_samples,
>, <Line: +                              max_iter=max_iter, tol=tol, random_state=77,
>, <Line: +                              fit_intercept=fit_intercept,
>, <Line: +                              class_weight=class_weight)
>, <Line: +    clf2 = clone(clf1)
>, <Line: +    clf2.fit(sp.csr_matrix(X), y)
>, <Line: +    le = LabelEncoder()
>, <Line: +    class_weight_ = compute_class_weight(class_weight, np.unique(y), y)
>, <Line: +    sample_weight = class_weight_[le.fit_transform(y)]
>, <Line: +    coef1 = []
>, <Line: +    intercept1 = []
>, <Line: +    for cl in classes:
>, <Line: +        spweights1, spintercept1 = sag_sparse(X, y_encoded, step_size, alpha,
>, <Line: +                                              n_iter=max_iter, dloss=log_dloss,
>, <Line: +                                              sample_weight=sample_weight)
>, <Line: +        spweights2, spintercept2 = sag_sparse(X, y_encoded, step_size, alpha,
>, <Line: +                                              n_iter=max_iter, dloss=log_dloss,
>, <Line: +                                              sample_weight=sample_weight,
>, <Line: +        coef1.append(spweights1)
>, <Line: +        intercept1.append(spintercept1)
>, <Line: +    coef1 = np.vstack(coef1)
>, <Line: +    intercept1 = np.array(intercept1)
>, <Line: +                                  coef1[i].ravel(),
>, <Line: +        assert_almost_equal(clf1.intercept_[i], intercept1[i], decimal=1)
>, <Line: +    """tests if ValueError is thrown with only one class"""
>, <Line: +    y = [1, 1]
>, <Line: +    assert_raise_message(ValueError,
>, <Line: +                         "This solver needs samples of at least 2 classes "
>, <Line: +                         "in the data",
>, <Line: +                         LogisticRegression(solver='sag').fit,
>, <Line: +                         X, y)
>, <Line: +def test_step_size_alpha_error():
>, <Line: +    X = [[0, 0], [0, 0]]
>, <Line: +    y = [1, -1]
>, <Line: +    fit_intercept = False
>, <Line: +    alpha = 1.
>, <Line: +    msg = ("Current sag implementation does not handle the case"
>, <Line: +           " step_size * alpha_scaled == 1")
>, <Line: +    clf1 = LogisticRegression(solver='sag', C=1. / alpha,
>, <Line: +                              fit_intercept=fit_intercept)
>, <Line: +    assert_raise_message(ZeroDivisionError, msg, clf1.fit, X, y)
>, <Line: +    clf2 = Ridge(fit_intercept=fit_intercept, solver='sag', alpha=alpha)
>, <Line: +    assert_raise_message(ZeroDivisionError, msg, clf2.fit, X, y)
>]
[<Line: -import math
>, <Line: -from sklearn.linear_model import SAGRegressor, SAGClassifier
>, <Line: -from sklearn.utils.testing import assert_raises_regexp
>, <Line: -class SparseSAGClassifier(SAGClassifier):
>, <Line: -    def fit(self, X, y, *args, **kw):
>, <Line: -        X = sp.csr_matrix(X)
>, <Line: -        return SAGClassifier.fit(self, X, y, *args, **kw)
>, <Line: -    def partial_fit(self, X, y, *args, **kw):
>, <Line: -        X = sp.csr_matrix(X)
>, <Line: -        return SAGClassifier.partial_fit(self, X, y, *args, **kw)
>, <Line: -    def decision_function(self, X):
>, <Line: -        X = sp.csr_matrix(X)
>, <Line: -        return SAGClassifier.decision_function(self, X)
>, <Line: -    def predict_proba(self, X):
>, <Line: -        X = sp.csr_matrix(X)
>, <Line: -        return SAGClassifier.predict_proba(self, X)
>, <Line: -class SparseSAGRegressor(SAGRegressor):
>, <Line: -    def fit(self, X, y, *args, **kw):
>, <Line: -        X = sp.csr_matrix(X)
>, <Line: -        return SAGRegressor.fit(self, X, y, *args, **kw)
>, <Line: -    def partial_fit(self, X, y, *args, **kw):
>, <Line: -        X = sp.csr_matrix(X)
>, <Line: -        return SAGRegressor.partial_fit(self, X, y, *args, **kw)
>, <Line: -    def decision_function(self, X, *args, **kw):
>, <Line: -        X = sp.csr_matrix(X)
>, <Line: -        return SAGRegressor.decision_function(self, X, *args, **kw)
>, <Line: -def sag(X, y, eta, alpha, n_iter=1, dloss=None, sparse=False,
>, <Line: -        class_weight=None, fit_intercept=True):
>, <Line: -            if class_weight:
>, <Line: -                gradient *= class_weight[y[idx]]
>, <Line: -                intercept -= eta * intercept_sum_gradient / len(seen) * decay
>, <Line: -            weights -= eta * sum_gradient / len(seen)
>, <Line: -def sag_sparse(X, y, eta, alpha, n_iter=1,
>, <Line: -               dloss=None, class_weight=None, sparse=False,
>, <Line: -            if class_weight:
>, <Line: -                gradient *= class_weight[y[idx]]
>, <Line: -                intercept -= eta * (intercept_sum_gradient / len(seen)) * decay
>, <Line: -            wscale *= (1.0 - alpha * eta)
>, <Line: -                c_sum[0] = eta / (wscale * len(seen))
>, <Line: -                                  eta / (wscale * len(seen)))
>, <Line: -def get_eta(X, alpha, fit_intercept, classification=True):
>, <Line: -    n_samples = 40
>, <Line: -    n_iter = 100
>, <Line: -    eta = get_eta(X, alpha, fit_intercept)
>, <Line: -    clf = SAGClassifier(fit_intercept=fit_intercept, tol=.00000000001,
>, <Line: -                        alpha=alpha, max_iter=n_iter, random_state=10)
>, <Line: -    weights, intercept = sag_sparse(X, y, eta, alpha, n_iter=n_iter,
>, <Line: -    weights2, intercept2 = sag(X, y, eta, alpha, n_iter=n_iter,
>, <Line: -    n_samples = 30
>, <Line: -    n_features = 10
>, <Line: -    np.random.seed(10)
>, <Line: -    X = np.random.random((n_samples, n_features))
>, <Line: -    true_w = np.random.random(n_features)
>, <Line: -    alpha = 1.1
>, <Line: -    eta = get_eta(X, alpha, fit_intercept, classification=False)
>, <Line: -    clf = SAGRegressor(fit_intercept=fit_intercept, tol=.00000000001,
>, <Line: -                       alpha=alpha, max_iter=n_iter, random_state=10)
>, <Line: -    weights, intercept = sag_sparse(X, y, eta, alpha, n_iter=n_iter,
>, <Line: -                                    dloss=squared_dloss,
>, <Line: -                                    fit_intercept=fit_intercept)
>, <Line: -    weights2, intercept2 = sag(X, y, eta, alpha, n_iter=n_iter,
>, <Line: -    weights = np.atleast_2d(weights)
>, <Line: -    intercept = np.atleast_1d(intercept)
>, <Line: -    weights2 = np.atleast_2d(weights2)
>, <Line: -    intercept2 = np.atleast_1d(intercept2)
>, <Line: -    assert_array_almost_equal(weights, clf.coef_, decimal=10)
>, <Line: -    assert_array_almost_equal(intercept, clf.intercept_, decimal=10)
>, <Line: -    n_samples = 500
>, <Line: -    n_iter = 20
>, <Line: -    clf1 = SAGClassifier(fit_intercept=False, tol=.0000001, alpha=alpha,
>, <Line: -                         max_iter=n_iter, random_state=10)
>, <Line: -    clf2 = SparseSAGClassifier(fit_intercept=False, tol=.0000001, alpha=alpha,
>, <Line: -                               max_iter=n_iter, random_state=10)
>, <Line: -                              C=1.0 / (alpha * n_samples), max_iter=n_iter,
>, <Line: -    clf2.fit(X, y)
>, <Line: -    n_samples = 500
>, <Line: -    np.random.seed(10)
>, <Line: -    X = np.random.random((n_samples, n_features))
>, <Line: -    true_w = np.random.random(n_features)
>, <Line: -    clf1 = SAGRegressor(fit_intercept=False, tol=.00001, alpha=alpha,
>, <Line: -                        max_iter=n_iter, random_state=10)
>, <Line: -    clf2 = SparseSAGRegressor(fit_intercept=False, tol=.00001, alpha=alpha,
>, <Line: -                              max_iter=n_iter, random_state=10)
>, <Line: -    clf3 = Ridge(fit_intercept=False, tol=.00001,
>, <Line: -                 alpha=alpha * n_samples, max_iter=n_iter, solver="lsqr")
>, <Line: -    clf2.fit(X, y)
>, <Line: -    pobj1 = get_pobj(clf1.coef_, alpha, X, y, log_loss)
>, <Line: -    pobj2 = get_pobj(clf2.coef_, alpha, X, y, log_loss)
>, <Line: -    pobj3 = get_pobj(clf3.coef_, alpha, X, y, log_loss)
>, <Line: -    assert_array_almost_equal(pobj2, pobj3, decimal=4)
>, <Line: -    assert_array_almost_equal(pobj3, pobj1, decimal=4)
>, <Line: -    """tests the if the sag regressor computed correctly"""
>, <Line: -    eta = .001
>, <Line: -    n_features = 20
>, <Line: -    n_samples = 100
>, <Line: -    max_iter = 1000
>, <Line: -    tol = .001
>, <Line: -    n_iter = 51
>, <Line: -    clf1 = SAGRegressor(eta0=eta, alpha=alpha,
>, <Line: -                        max_iter=max_iter, tol=tol, random_state=77)
>, <Line: -    clf2 = SparseSAGRegressor(eta0=eta, alpha=alpha,
>, <Line: -                              max_iter=max_iter, tol=tol, random_state=77)
>, <Line: -    y = np.dot(X, w)
>, <Line: -    clf2.fit(X, y)
>, <Line: -    spweights, spintercept = sag_sparse(X, y, eta, alpha,
>, <Line: -                                        n_iter=n_iter,
>, <Line: -                                        dloss=squared_dloss)
>, <Line: -    spweights2, spintercept2 = sag_sparse(X, y, eta, alpha,
>, <Line: -                                          n_iter=n_iter,
>, <Line: -                                          dloss=squared_dloss, sparse=True)
>, <Line: -                              spweights.ravel(),
>, <Line: -                              decimal=2)
>, <Line: -    assert_almost_equal(clf1.intercept_, spintercept, decimal=1)
>, <Line: -    assert_array_almost_equal(clf2.coef_.ravel(),
>, <Line: -                              spweights2.ravel(),
>, <Line: -                              decimal=2)
>, <Line: -    assert_almost_equal(clf2.intercept_, spintercept2, decimal=1)
>, <Line: -def test_regressor_warm_start():
>, <Line: -    """tests the regressor warmstart"""
>, <Line: -    eta = .001
>, <Line: -    alpha = .1
>, <Line: -    n_features = 20
>, <Line: -    n_samples = 100
>, <Line: -    n_iter = 53
>, <Line: -    max_iter = 1000
>, <Line: -    tol = .001
>, <Line: -    clf1 = SAGRegressor(eta0=eta, alpha=alpha, warm_start=True,
>, <Line: -                        max_iter=max_iter, tol=tol, random_state=77)
>, <Line: -    clf2 = SparseSAGRegressor(eta0=eta, alpha=alpha, warm_start=True,
>, <Line: -                              max_iter=max_iter, tol=tol, random_state=77)
>, <Line: -    rng = np.random.RandomState(0)
>, <Line: -    X = rng.normal(size=(n_samples, n_features))
>, <Line: -    w = rng.normal(size=n_features)
>, <Line: -    y = np.dot(X, w)
>, <Line: -    clf1.fit(X, y)
>, <Line: -    clf1.fit(X, y)
>, <Line: -    clf2.fit(X, y)
>, <Line: -    clf2.fit(X, y)
>, <Line: -    spweights, spintercept = sag_sparse(X, y, eta, alpha,
>, <Line: -                                        n_iter=n_iter,
>, <Line: -                                        dloss=squared_dloss)
>, <Line: -    spweights2, spintercept2 = sag_sparse(X, y, eta, alpha,
>, <Line: -                                          n_iter=n_iter,
>, <Line: -                                          dloss=squared_dloss, sparse=True)
>, <Line: -    assert_array_almost_equal(clf1.coef_.ravel(),
>, <Line: -                              spweights.ravel(),
>, <Line: -                              decimal=2)
>, <Line: -    assert_almost_equal(clf1.intercept_, spintercept, decimal=1)
>, <Line: -    assert_array_almost_equal(clf2.coef_.ravel(),
>, <Line: -                              spweights.ravel(),
>, <Line: -                              decimal=2)
>, <Line: -    assert_almost_equal(clf2.intercept_, spintercept, decimal=1)
>, <Line: -def test_auto_eta():
>, <Line: -    """tests the auto eta computed correctly"""
>, <Line: -    X = np.array([[1, 2, 3], [2, 3, 4], [2, 3, 2]])
>, <Line: -    y = [.5, .6, .7]
>, <Line: -    alpha = 1.0
>, <Line: -    n_iter = 8
>, <Line: -    sp_n_iter = 18
>, <Line: -    tol = .01
>, <Line: -    max_iter = 1000
>, <Line: -    # sum the squares of the second sample because that's the largest
>, <Line: -    eta = 4 + 9 + 16
>, <Line: -    eta = 1.0 / (eta + alpha)
>, <Line: -    clf1 = SAGRegressor(eta0='auto', alpha=alpha,
>, <Line: -                        max_iter=max_iter, tol=tol, random_state=77)
>, <Line: -    clf2 = SparseSAGRegressor(eta0='auto', alpha=alpha,
>, <Line: -                              max_iter=max_iter, tol=.001,
>, <Line: -                              random_state=77)
>, <Line: -    clf1.fit(X, y)
>, <Line: -    clf2.fit(X, y)
>, <Line: -    spweights, spintercept = sag_sparse(X, y, eta, alpha,
>, <Line: -                                        n_iter=n_iter,
>, <Line: -                                        dloss=squared_dloss)
>, <Line: -    spweights2, spintercept2 = sag_sparse(X, y, eta, alpha,
>, <Line: -                                          n_iter=sp_n_iter,
>, <Line: -                                          dloss=squared_dloss, sparse=True)
>, <Line: -    assert_array_almost_equal(clf1.coef_.ravel(),
>, <Line: -                              spweights.ravel(),
>, <Line: -                              decimal=2)
>, <Line: -    assert_almost_equal(clf1.intercept_, spintercept, decimal=1)
>, <Line: -    assert_array_almost_equal(clf2.coef_.ravel(),
>, <Line: -                              spweights2.ravel(),
>, <Line: -                              decimal=2)
>, <Line: -    assert_almost_equal(clf2.intercept_, spintercept2, decimal=1)
>, <Line: -    """tests the if the sag regressor performs well"""
>, <Line: -    n_samples = 100
>, <Line: -    clf = SAGRegressor(eta0=.001, alpha=0.1, max_iter=max_iter, tol=tol)
>, <Line: -    clf2 = SparseSAGRegressor(eta0=.001, alpha=0.1, max_iter=max_iter, tol=tol)
>, <Line: -    clf.fit(X, y)
>, <Line: -    clf2.fit(X, y)
>, <Line: -    score = clf.score(X, y)
>, <Line: -    assert_greater(score, 0.99)
>, <Line: -    clf = SAGRegressor(eta0=.001, alpha=0.1, max_iter=max_iter, tol=tol)
>, <Line: -    clf2 = SparseSAGRegressor(eta0=.001, alpha=0.1, max_iter=max_iter, tol=tol)
>, <Line: -    clf.fit(X, y)
>, <Line: -    clf2.fit(X, y)
>, <Line: -    score = clf.score(X, y)
>, <Line: -    assert_greater(score, 0.5)
>, <Line: -    """tests the binary classifier computed correctly"""
>, <Line: -    eta = .001
>, <Line: -    n_iter = 59
>, <Line: -    tol = .01
>, <Line: -    max_iter = 1000
>, <Line: -    clf1 = SAGClassifier(eta0=eta, alpha=alpha,
>, <Line: -                         max_iter=max_iter, tol=tol, random_state=77)
>, <Line: -    clf2 = SparseSAGClassifier(eta0=eta, alpha=alpha,
>, <Line: -                               max_iter=max_iter, tol=tol, random_state=77)
>, <Line: -    clf2.fit(X, y)
>, <Line: -    spweights, spintercept = sag_sparse(X, y, eta, alpha,
>, <Line: -                                        n_iter=n_iter,
>, <Line: -                                        dloss=log_dloss)
>, <Line: -    spweights2, spintercept2 = sag_sparse(X, y, eta, alpha,
>, <Line: -                                          dloss=log_dloss, sparse=True)
>, <Line: -                              decimal=1)
>, <Line: -                              decimal=1)
>, <Line: -    """tests the multiclass classifier is computed correctly"""
>, <Line: -    eta = .001
>, <Line: -    n_samples = 50
>, <Line: -    tol = .01
>, <Line: -    max_iter = 1000
>, <Line: -    clf1 = SAGClassifier(eta0=eta, alpha=alpha,
>, <Line: -                         max_iter=max_iter, tol=tol, random_state=77)
>, <Line: -    clf2 = SparseSAGClassifier(eta0=eta, alpha=alpha,
>, <Line: -                               max_iter=max_iter, tol=tol, random_state=77)
>, <Line: -    y[y == 0] = -1
>, <Line: -    itrs = [84, 53, 49]
>, <Line: -    sp_itrs = [48, 55, 49]
>, <Line: -    clf2.fit(X, y)
>, <Line: -    coef = []
>, <Line: -    intercept = []
>, <Line: -    for cl, it, sp_it in zip(classes, itrs, sp_itrs):
>, <Line: -        spweights, spintercept = sag_sparse(X, y_encoded, eta, alpha,
>, <Line: -                                            dloss=log_dloss,
>, <Line: -                                            n_iter=it)
>, <Line: -        spweights2, spintercept2 = sag_sparse(X, y_encoded, eta, alpha,
>, <Line: -                                              dloss=log_dloss,
>, <Line: -                                              n_iter=sp_it, sparse=True)
>, <Line: -        coef.append(spweights)
>, <Line: -        intercept.append(spintercept)
>, <Line: -    coef = np.vstack(coef)
>, <Line: -    intercept = np.array(intercept)
>, <Line: -                                  coef[i].ravel(),
>, <Line: -        assert_almost_equal(clf1.intercept_[i], intercept[i], decimal=1)
>, <Line: -    """tests classifier results match target"""
>, <Line: -    eta = .2
>, <Line: -    max_iter = 2000
>, <Line: -    clf = SAGClassifier(eta0=eta, alpha=alpha, max_iter=max_iter, tol=tol)
>, <Line: -    clf2 = SparseSAGClassifier(eta0=eta, alpha=alpha,
>, <Line: -                               max_iter=max_iter, tol=tol)
>, <Line: -    clf.fit(X, y)
>, <Line: -    clf2.fit(X, y)
>, <Line: -    pred = clf.predict(X)
>, <Line: -    pred2 = clf2.predict(X)
>, <Line: -    assert_almost_equal(pred, y, decimal=12)
>, <Line: -    assert_almost_equal(pred2, y, decimal=12)
>, <Line: -def test_binary_classifier_warm_start():
>, <Line: -    """tests binary classifier with a warm start"""
>, <Line: -    eta = .001
>, <Line: -    alpha = .1
>, <Line: -    n_samples = 50
>, <Line: -    n_iter = 59
>, <Line: -    tol = .01
>, <Line: -    max_iter = 2000
>, <Line: -    clf1 = SAGClassifier(eta0=eta, alpha=alpha,
>, <Line: -                         max_iter=max_iter, tol=tol, random_state=77,
>, <Line: -                         warm_start=True)
>, <Line: -    clf2 = SparseSAGClassifier(eta0=eta, alpha=alpha, max_iter=max_iter,
>, <Line: -                               tol=tol, random_state=77, warm_start=True)
>, <Line: -    X, y = make_blobs(n_samples=n_samples, centers=2, random_state=0,
>, <Line: -                      cluster_std=0.1)
>, <Line: -    classes = np.unique(y)
>, <Line: -    y_tmp = np.ones(n_samples)
>, <Line: -    y_tmp[y != classes[1]] = -1
>, <Line: -    y = y_tmp
>, <Line: -    clf1.fit(X, y)
>, <Line: -    clf1.fit(X, y)
>, <Line: -    clf2.fit(X, y)
>, <Line: -    clf2.fit(X, y)
>, <Line: -    spweights, spintercept = sag_sparse(X, y, eta, alpha,
>, <Line: -                                        n_iter=n_iter,
>, <Line: -                                        dloss=log_dloss)
>, <Line: -    spweights2, spintercept2 = sag_sparse(X, y, eta, alpha,
>, <Line: -                                          n_iter=61,
>, <Line: -                                          dloss=log_dloss, sparse=True)
>, <Line: -    assert_array_almost_equal(clf1.coef_.ravel(),
>, <Line: -                              spweights.ravel(),
>, <Line: -                              decimal=2)
>, <Line: -    assert_almost_equal(clf1.intercept_, spintercept, decimal=1)
>, <Line: -    assert_array_almost_equal(clf2.coef_.ravel(),
>, <Line: -                              spweights2.ravel(),
>, <Line: -                              decimal=2)
>, <Line: -    assert_almost_equal(clf2.intercept_, spintercept2, decimal=1)
>, <Line: -def test_multiclass_classifier_warm_start():
>, <Line: -    """tests multiclass classifier with a warm start"""
>, <Line: -    eta = .001
>, <Line: -    alpha = .1
>, <Line: -    n_samples = 20
>, <Line: -    tol = .01
>, <Line: -    max_iter = 3000
>, <Line: -    clf1 = SAGClassifier(eta0=eta, alpha=alpha,
>, <Line: -                         max_iter=max_iter, tol=tol, random_state=77,
>, <Line: -                         warm_start=True)
>, <Line: -    clf2 = SparseSAGClassifier(eta0=eta, alpha=alpha,
>, <Line: -                               max_iter=max_iter, tol=tol, random_state=77,
>, <Line: -                               warm_start=True)
>, <Line: -    X, y = make_blobs(n_samples=n_samples, centers=3, random_state=0,
>, <Line: -                      cluster_std=0.1)
>, <Line: -    classes = np.unique(y)
>, <Line: -    clf1.fit(X, y)
>, <Line: -    clf2.fit(X, y)
>, <Line: -    clf2.fit(X, y)
>, <Line: -    itrs = [65, 63, 66]
>, <Line: -    sp_itrs = [39, 63, 66]
>, <Line: -    coef = []
>, <Line: -    intercept = []
>, <Line: -    coef2 = []
>, <Line: -    intercept2 = []
>, <Line: -    for cl, it, sp_it in zip(classes, itrs, sp_itrs):
>, <Line: -        y_encoded = np.ones(n_samples)
>, <Line: -        y_encoded[y != cl] = -1
>, <Line: -        spweights, spintercept = sag_sparse(X, y_encoded, eta, alpha,
>, <Line: -                                            n_iter=it,
>, <Line: -                                            dloss=log_dloss)
>, <Line: -        spweights2, spintercept2 = sag_sparse(X, y_encoded, eta, alpha,
>, <Line: -                                              n_iter=sp_it,
>, <Line: -                                              dloss=log_dloss, sparse=True)
>, <Line: -        coef.append(spweights)
>, <Line: -        intercept.append(spintercept)
>, <Line: -        coef2.append(spweights2)
>, <Line: -        intercept2.append(spintercept2)
>, <Line: -    coef = np.vstack(coef)
>, <Line: -    intercept = np.array(intercept)
>, <Line: -    coef2 = np.vstack(coef2)
>, <Line: -    intercept2 = np.array(intercept2)
>, <Line: -    for i, cl in enumerate(classes):
>, <Line: -        assert_array_almost_equal(clf1.coef_[i].ravel(),
>, <Line: -                                  coef[i].ravel(),
>, <Line: -                                  decimal=2)
>, <Line: -        assert_almost_equal(clf1.intercept_[i], intercept[i], decimal=1)
>, <Line: -        assert_array_almost_equal(clf2.coef_[i].ravel(),
>, <Line: -                                  coef2[i].ravel(),
>, <Line: -                                  decimal=2)
>, <Line: -        assert_almost_equal(clf2.intercept_[i], intercept2[i], decimal=1)
>, <Line: -    eta = .001
>, <Line: -    n_iter = 9
>, <Line: -    tol = .1
>, <Line: -    max_iter = 1000
>, <Line: -    X, y = make_blobs(n_samples=n_samples, centers=2, random_state=0,
>, <Line: -    clf1 = SAGClassifier(eta0=eta, alpha=alpha, max_iter=max_iter, tol=tol,
>, <Line: -                         random_state=77, warm_start=True,
>, <Line: -                         class_weight=class_weight)
>, <Line: -    clf2 = SparseSAGClassifier(eta0=eta, alpha=alpha, max_iter=max_iter,
>, <Line: -                               tol=tol, random_state=77, warm_start=True,
>, <Line: -                               class_weight=class_weight)
>, <Line: -    clf2.fit(X, y)
>, <Line: -    spweights, spintercept = sag_sparse(X, y, eta, alpha,
>, <Line: -                                        n_iter=n_iter,
>, <Line: -                                        class_weight=class_weight)
>, <Line: -    spweights2, spintercept2 = sag_sparse(X, y, eta, alpha,
>, <Line: -                                          dloss=log_dloss,
>, <Line: -                                          class_weight=class_weight,
>, <Line: -                                          sparse=True)
>, <Line: -    eta = .001
>, <Line: -    tol = .1
>, <Line: -    max_iter = 1000
>, <Line: -    clf1 = SAGClassifier(eta0=eta, alpha=alpha, max_iter=max_iter, tol=tol,
>, <Line: -                         random_state=77, warm_start=True,
>, <Line: -                         class_weight=class_weight)
>, <Line: -    clf2 = SparseSAGClassifier(eta0=eta, alpha=alpha, max_iter=max_iter,
>, <Line: -                               tol=tol, random_state=77, warm_start=True,
>, <Line: -                               class_weight=class_weight)
>, <Line: -    clf2.fit(X, y)
>, <Line: -    itrs = [9, 9, 10]
>, <Line: -    coef = []
>, <Line: -    intercept = []
>, <Line: -    for cl, it in zip(classes, itrs):
>, <Line: -        cl_weight = {-1: 1.0, 1: 1.0}
>, <Line: -        cl_weight[1] = class_weight[cl]
>, <Line: -        spweights, spintercept = sag_sparse(X, y_encoded, eta, alpha,
>, <Line: -                                            n_iter=it, dloss=log_dloss,
>, <Line: -                                            class_weight=cl_weight)
>, <Line: -        spweights2, spintercept2 = sag_sparse(X, y_encoded, eta, alpha,
>, <Line: -                                              n_iter=it, dloss=log_dloss,
>, <Line: -                                              class_weight=cl_weight,
>, <Line: -        coef.append(spweights)
>, <Line: -        intercept.append(spintercept)
>, <Line: -    coef = np.vstack(coef)
>, <Line: -    intercept = np.array(intercept)
>, <Line: -                                  coef[i].ravel(),
>, <Line: -        assert_almost_equal(clf1.intercept_[i], intercept[i], decimal=1)
>, <Line: -    """tests value error thrown with only one class"""
>, <Line: -    Y = [1, 1]
>, <Line: -    assert_raises_regexp(ValueError,
>, <Line: -                         "The number of class labels must be "
>, <Line: -                         "greater than one.",
>, <Line: -                         SAGClassifier().fit,
>, <Line: -                         X, Y)
>]