[<Line: +from .sag import sag_solver
>, <Line: +from .sag_fast import get_max_squared_sum
>, <Line: +from ..utils.validation import (DataConversionWarning,
>, <Line: +    if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag']:
>, <Line: +                         " newton-cg, lbfgs and sag solvers, got %s" % solver)
>, <Line: +    if multi_class == 'multinomial' and solver in ['liblinear', 'sag']:
>, <Line: +                             solver='lbfgs', coef=None, copy=False,
>, <Line: +                             random_state=None, check_input=True,
>, <Line: +                             max_squared_sum=None):
>, <Line: +    solver : {'lbfgs', 'newton-cg', 'liblinear', 'sag'}
>, <Line: +        Useless for liblinear solver.
>, <Line: +    copy : bool, default False
>, <Line: +        Whether or not to produce a copy of the data. A copy is not required
>, <Line: +        anymore. This parameter is deprecated and will be removed in 0.19.
>, <Line: +        Used to specify the norm used in the penalization. The 'newton-cg',
>, <Line: +        'sag' and 'lbfgs' solvers support only l2 penalties.
>, <Line: +    check_input : bool, default True
>, <Line: +        If False, the input arrays X and y will not be checked.
>, <Line: +    max_squared_sum : float, default None
>, <Line: +        Maximum squared sum of X over samples. Used only in SAG solver.
>, <Line: +        If None, it will be computed, going through all the samples.
>, <Line: +        The value should be precomputed to speed up cross validation.
>, <Line: +    n_iter : array, shape (n_cs,)
>, <Line: +        Actual number of iteration for each Cs.
>, <Line: +    if copy:
>, <Line: +        warnings.warn("A copy is not required anymore. The 'copy' parameter "
>, <Line: +                      "is deprecated and will be removed in 0.19.",
>, <Line: +                      DeprecationWarning)
>, <Line: +    if check_input or copy:
>, <Line: +        X = check_array(X, accept_sparse='csr', dtype=np.float64)
>, <Line: +        y = check_array(y, ensure_2d=False, copy=copy, dtype=None)
>, <Line: +        check_consistent_length(X, y)
>, <Line: +                                 "newton-cg or sag solvers or set "
>, <Line: +        mask_classes = np.array([-1, 1])
>, <Line: +        y_bin = np.ones(y.shape, dtype=np.float64)
>, <Line: +        y_bin[~mask] = -1.
>, <Line: +        class_weight_ = compute_class_weight(class_weight, mask_classes,
>, <Line: +                                             y_bin)
>, <Line: +        sample_weight = class_weight_[le.fit_transform(y_bin)]
>, <Line: +        target = y_bin
>, <Line: +    warm_start_sag = {'coef': w0}
>, <Line: +    n_iter = np.zeros(len(Cs), dtype=np.int32)
>, <Line: +    for i, C in enumerate(Cs):
>, <Line: +            try:
>, <Line: +                n_iter_i = info['nit'] - 1
>, <Line: +            except:
>, <Line: +                n_iter_i = info['funcalls'] - 1
>, <Line: +            w0, n_iter_i = newton_cg(hess, func, grad, w0, args=args,
>, <Line: +                                     maxiter=max_iter, tol=tol)
>, <Line: +            coef_, intercept_, n_iter_i, = _fit_liblinear(
>, <Line: +                X, target, C, fit_intercept, intercept_scaling, class_weight,
>, <Line: +        elif solver == 'sag':
>, <Line: +            w0, n_iter_i, warm_start_sag = sag_solver(
>, <Line: +                X, target, sample_weight, 'log', 1. / C, max_iter, tol,
>, <Line: +                verbose, random_state, False, max_squared_sum,
>, <Line: +                warm_start_sag)
>, <Line: +                             "'newton-cg', 'sag'}, got '%s' instead" % solver)
>, <Line: +            coefs.append(w0.copy())
>, <Line: +        n_iter[i] = n_iter_i
>, <Line: +    return coefs, np.array(Cs), n_iter
>, <Line: +                          dual=False, intercept_scaling=1.,
>, <Line: +                          multi_class='ovr', random_state=None,
>, <Line: +                          max_squared_sum=None):
>, <Line: +    solver : {'lbfgs', 'newton-cg', 'liblinear', 'sag'}
>, <Line: +        the entire probability distribution. Works only for the 'lbfgs' and
>, <Line: +        'newton-cg' solver.
>, <Line: +    random_state : int seed, RandomState instance, or None (default)
>, <Line: +        The seed of the pseudo random number generator to use when
>, <Line: +        shuffling the data.
>, <Line: +    max_squared_sum : float, default None
>, <Line: +        Maximum squared sum of X over samples. Used only in SAG solver.
>, <Line: +        If None, it will be computed, going through all the samples.
>, <Line: +        The value should be precomputed to speed up cross validation.
>, <Line: +    n_iter : array, shape(n_cs,)
>, <Line: +        Actual number of iteration for each Cs.
>, <Line: +    coefs, Cs, n_iter = logistic_regression_path(
>, <Line: +        X_train, y_train, Cs=Cs, fit_intercept=fit_intercept,
>, <Line: +        solver=solver, max_iter=max_iter, class_weight=class_weight,
>, <Line: +        pos_class=pos_class, multi_class=multi_class,
>, <Line: +        tol=tol, verbose=verbose, dual=dual, penalty=penalty,
>, <Line: +        intercept_scaling=intercept_scaling, random_state=random_state,
>, <Line: +        check_input=False, max_squared_sum=max_squared_sum)
>, <Line: +    log_reg = LogisticRegression(fit_intercept=fit_intercept)
>, <Line: +        y_test = np.ones(y_test.shape, dtype=np.float64)
>, <Line: +        y_test[~mask] = -1.
>, <Line: +    y_test = check_array(y_test, dtype=np.float64, ensure_2d=False)
>, <Line: +    return coefs, Cs, np.array(scores), n_iter
>, <Line: +        added to the decision function.
>, <Line: +        Useful only for the newton-cg, sag and lbfgs solvers.
>, <Line: +        Maximum number of iterations taken for the solvers to converge.
>, <Line: +    solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag'}
>, <Line: +        - For small datasets, 'liblinear' is a good choice, whereas 'sag' is
>, <Line: +            faster for large ones.
>, <Line: +        - For multiclass problems, only 'newton-cg' and 'lbfgs' handle
>, <Line: +            multinomial loss; 'sag' and 'liblinear' are limited to
>, <Line: +            one-versus-rest schemes.
>, <Line: +        - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty.
>, <Line: +        Note that 'sag' fast convergence is only guaranteed on features with
>, <Line: +        approximately the same scale. You can preprocess the data with a
>, <Line: +        scaler from sklearn.preprocessing.
>, <Line: +    warm_start : bool, optional
>, <Line: +        When set to True, reuse the solution of the previous call to fit as
>, <Line: +        initialization, otherwise, just erase the previous solution.
>, <Line: +        Useless for liblinear solver.
>, <Line: +    n_jobs : int, optional
>, <Line: +        Number of CPU cores used during the cross-validation loop. If given
>, <Line: +        a value of -1, all cores are used.
>, <Line: +    n_iter_ : array, shape (n_classes,) or (1, )
>, <Line: +        Actual number of iterations for all classes. If binary or multinomial,
>, <Line: +        it returns only 1 element. For liblinear solver, only the maximum
>, <Line: +        number of iteration across all classes is given.
>, <Line: +                 multi_class='ovr', verbose=0, warm_start=False, n_jobs=1):
>, <Line: +        self.warm_start = warm_start
>, <Line: +        self.n_jobs = n_jobs
>, <Line: +        n_samples, n_features = X.shape
>, <Line: +            self.coef_, self.intercept_, n_iter_ = _fit_liblinear(
>, <Line: +            self.n_iter_ = np.array([n_iter_])
>, <Line: +        max_squared_sum = get_max_squared_sum(X) if self.solver == 'sag' \
>, <Line: +            else None
>, <Line: +        if self.warm_start:
>, <Line: +            warm_start_coef = getattr(self, 'coef_', None)
>, <Line: +        else:
>, <Line: +            warm_start_coef = None
>, <Line: +        if warm_start_coef is not None and self.fit_intercept:
>, <Line: +            warm_start_coef = np.append(warm_start_coef,
>, <Line: +                                        self.intercept_[:, np.newaxis],
>, <Line: +                                        axis=1)
>, <Line: +            warm_start_coef = [warm_start_coef]
>, <Line: +        if warm_start_coef is None:
>, <Line: +            warm_start_coef = [None] * n_classes
>, <Line: +        path_func = delayed(logistic_regression_path)
>, <Line: +        # The SAG solver releases the GIL so it's more efficient to use
>, <Line: +        # threads for this solver.
>, <Line: +        backend = 'threading' if self.solver == 'sag' else 'multiprocessing'
>, <Line: +        fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
>, <Line: +                               backend=backend)(
>, <Line: +            path_func(X, y, pos_class=class_, Cs=[self.C],
>, <Line: +                      fit_intercept=self.fit_intercept, tol=self.tol,
>, <Line: +                      verbose=self.verbose, solver=self.solver, copy=False,
>, <Line: +                      multi_class=self.multi_class, max_iter=self.max_iter,
>, <Line: +                      class_weight=self.class_weight, check_input=False,
>, <Line: +                      random_state=self.random_state, coef=warm_start_coef_,
>, <Line: +                      max_squared_sum=max_squared_sum)
>, <Line: +            for (class_, warm_start_coef_) in zip(classes_, warm_start_coef))
>, <Line: +        fold_coefs_, _, n_iter_ = zip(*fold_coefs_)
>, <Line: +        self.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, 0]
>, <Line: +        if self.multi_class == 'multinomial':
>, <Line: +            self.coef_ = fold_coefs_[0][0]
>, <Line: +        else:
>, <Line: +            self.coef_ = np.asarray(fold_coefs_)
>, <Line: +            self.coef_ = self.coef_.reshape(n_classes, n_features +
>, <Line: +                                            int(self.fit_intercept))
>, <Line: +    This class implements logistic regression using liblinear, newton-cg, sag
>, <Line: +    of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2
>, <Line: +        added to the decision function.
>, <Line: +    solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag'}
>, <Line: +        - For small datasets, 'liblinear' is a good choice, whereas 'sag' is
>, <Line: +            faster for large ones.
>, <Line: +        - For multiclass problems, only 'newton-cg' and 'lbfgs' handle
>, <Line: +            multinomial loss; 'sag' and 'liblinear' are limited to
>, <Line: +            one-versus-rest schemes.
>, <Line: +        - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty.
>, <Line: +        - 'liblinear' might be slower in LogisticRegressionCV because it does
>, <Line: +            not handle warm-starting.
>, <Line: +        For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any
>, <Line: +        positive number for verbosity.
>, <Line: +        the entire probability distribution. Works only for 'lbfgs' and
>, <Line: +        'newton-cg' solvers.
>, <Line: +    random_state : int seed, RandomState instance, or None (default)
>, <Line: +        The seed of the pseudo random number generator to use when
>, <Line: +        shuffling the data.
>, <Line: +    n_iter_ : array, shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)
>, <Line: +        Actual number of iterations for all classes, folds and Cs.
>, <Line: +        In the binary or multinomial cases, the first dimension is equal to 1.
>, <Line: +                 refit=True, intercept_scaling=1., multi_class='ovr',
>, <Line: +                 random_state=None):
>, <Line: +        self.random_state = random_state
>, <Line: +        X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,
>, <Line: +                         order="C")
>, <Line: +        max_squared_sum = get_max_squared_sum(X) if self.solver == 'sag' \
>, <Line: +            else None
>, <Line: +        # The SAG solver releases the GIL so it's more efficient to use
>, <Line: +        # threads for this solver.
>, <Line: +        backend = 'threading' if self.solver == 'sag' else 'multiprocessing'
>, <Line: +        fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
>, <Line: +                               backend=backend)(
>, <Line: +                      intercept_scaling=self.intercept_scaling,
>, <Line: +                      random_state=self.random_state,
>, <Line: +                      max_squared_sum=max_squared_sum)
>, <Line: +            multi_coefs_paths, Cs, multi_scores, n_iter_ = zip(*fold_coefs_)
>, <Line: +            self.n_iter_ = np.reshape(n_iter_, (1, len(folds),
>, <Line: +                                      len(self.Cs_)))
>, <Line: +            coefs_paths, Cs, scores, n_iter_ = zip(*fold_coefs_)
>, <Line: +            self.n_iter_ = np.reshape(n_iter_, (n_classes, len(folds),
>, <Line: +                                      len(self.Cs_)))
>, <Line: +                w, _, _ = logistic_regression_path(
>, <Line: +                    penalty=self.penalty, copy=False,
>, <Line: +                    verbose=max(0, self.verbose - 1),
>, <Line: +                    random_state=self.random_state,
>, <Line: +                    check_input=False, max_squared_sum=max_squared_sum)
>]
[<Line: -from ..utils.validation import (as_float_array, DataConversionWarning,
>, <Line: -    if solver not in ['liblinear', 'newton-cg', 'lbfgs']:
>, <Line: -                         " newton-cg and lbfgs solvers, got %s" % solver)
>, <Line: -    if multi_class == 'multinomial' and solver == 'liblinear':
>, <Line: -                             solver='lbfgs', coef=None, copy=True,
>, <Line: -                             random_state=None):
>, <Line: -    solver : {'lbfgs', 'newton-cg', 'liblinear'}
>, <Line: -    copy : bool, default True
>, <Line: -        Whether or not to produce a copy of the data. Setting this to
>, <Line: -        True will be useful in cases, when logistic_regression_path
>, <Line: -        is called repeatedly with the same data, as y is modified
>, <Line: -        along the path.
>, <Line: -        Used to specify the norm used in the penalization. The newton-cg and
>, <Line: -        lbfgs solvers support only l2 penalties.
>, <Line: -    X = check_array(X, accept_sparse='csr', dtype=np.float64)
>, <Line: -    y = check_array(y, ensure_2d=False, copy=copy, dtype=None)
>, <Line: -    check_consistent_length(X, y)
>, <Line: -                                 "newton-cg solvers or set "
>, <Line: -        mask_classes = [-1, 1]
>, <Line: -        y[mask] = 1
>, <Line: -        y[~mask] = -1
>, <Line: -        # To take care of object dtypes, i.e 1 and -1 are in the form of
>, <Line: -        # strings.
>, <Line: -        y = as_float_array(y, copy=False)
>, <Line: -        class_weight_ = compute_class_weight(class_weight, mask_classes, y)
>, <Line: -        sample_weight = class_weight_[le.fit_transform(y)]
>, <Line: -        target = y
>, <Line: -    for C in Cs:
>, <Line: -            w0 = newton_cg(hess, func, grad, w0, args=args, maxiter=max_iter,
>, <Line: -                           tol=tol)
>, <Line: -            coef_, intercept_, _, = _fit_liblinear(
>, <Line: -                X, y, C, fit_intercept, intercept_scaling, class_weight,
>, <Line: -                             "'newton-cg'}, got '%s' instead" % solver)
>, <Line: -            coefs.append(w0)
>, <Line: -    return coefs, np.array(Cs)
>, <Line: -                          dual=False, copy=True, intercept_scaling=1.,
>, <Line: -                          multi_class='ovr'):
>, <Line: -    solver : {'lbfgs', 'newton-cg', 'liblinear'}
>, <Line: -        the entire probability distribution. Works only for the 'lbfgs'
>, <Line: -        solver.
>, <Line: -    copy : bool, default True
>, <Line: -        Whether or not to produce a copy of the data. Setting this to
>, <Line: -        True will be useful in cases, when ``_log_reg_scoring_path`` is called
>, <Line: -        repeatedly with the same data, as y is modified along the path.
>, <Line: -    log_reg = LogisticRegression(fit_intercept=fit_intercept)
>, <Line: -        y_test[mask] = 1
>, <Line: -        y_test[~mask] = -1
>, <Line: -    y_test = as_float_array(y_test, copy=False)
>, <Line: -    coefs, Cs = logistic_regression_path(X_train, y_train, Cs=Cs,
>, <Line: -                                         fit_intercept=fit_intercept,
>, <Line: -                                         solver=solver,
>, <Line: -                                         max_iter=max_iter,
>, <Line: -                                         class_weight=class_weight,
>, <Line: -                                         copy=copy, pos_class=pos_class,
>, <Line: -                                         multi_class=multi_class,
>, <Line: -                                         tol=tol, verbose=verbose,
>, <Line: -                                         dual=dual, penalty=penalty,
>, <Line: -                                         intercept_scaling=intercept_scaling)
>, <Line: -    return coefs, Cs, np.array(scores)
>, <Line: -        added the decision function.
>, <Line: -        Useful only for the newton-cg and lbfgs solvers. Maximum number of
>, <Line: -        iterations taken for the solvers to converge.
>, <Line: -    solver : {'newton-cg', 'lbfgs', 'liblinear'}
>, <Line: -    n_iter_ : int
>, <Line: -        Maximum of the actual number of iterations across all classes.
>, <Line: -        Valid only for the liblinear solver.
>, <Line: -                 multi_class='ovr', verbose=0):
>, <Line: -            self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(
>, <Line: -        for ind, class_ in enumerate(classes_):
>, <Line: -            coef_, _ = logistic_regression_path(
>, <Line: -                X, y, pos_class=class_, Cs=[self.C],
>, <Line: -                fit_intercept=self.fit_intercept, tol=self.tol,
>, <Line: -                verbose=self.verbose, solver=self.solver,
>, <Line: -                multi_class=self.multi_class, max_iter=self.max_iter,
>, <Line: -                class_weight=self.class_weight)
>, <Line: -            self.coef_.append(coef_[0])
>, <Line: -        self.coef_ = np.squeeze(self.coef_)
>, <Line: -        # For the binary case, this get squeezed to a 1-D array.
>, <Line: -        if self.coef_.ndim == 1:
>, <Line: -            self.coef_ = self.coef_[np.newaxis, :]
>, <Line: -        self.coef_ = np.asarray(self.coef_)
>, <Line: -    This class implements logistic regression using liblinear, newton-cg or
>, <Line: -    LBFGS optimizer. The newton-cg and lbfgs solvers support only L2
>, <Line: -        added the decision function.
>, <Line: -    solver : {'newton-cg', 'lbfgs', 'liblinear'}
>, <Line: -        For the liblinear and lbfgs solvers set verbose to any positive
>, <Line: -        number for verbosity.
>, <Line: -        the entire probability distribution. Works only for the 'lbfgs'
>, <Line: -        solver.
>, <Line: -                 refit=True, intercept_scaling=1., multi_class='ovr'):
>, <Line: -        X = check_array(X, accept_sparse='csr', dtype=np.float64)
>, <Line: -        y = check_array(y, ensure_2d=False, dtype=None)
>, <Line: -        fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(
>, <Line: -                      intercept_scaling=self.intercept_scaling
>, <Line: -                      )
>, <Line: -            multi_coefs_paths, Cs, multi_scores = zip(*fold_coefs_)
>, <Line: -            coefs_paths, Cs, scores = zip(*fold_coefs_)
>, <Line: -                w, _ = logistic_regression_path(
>, <Line: -                    penalty=self.penalty,
>, <Line: -                    verbose=max(0, self.verbose - 1))
>]