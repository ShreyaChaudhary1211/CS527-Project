[<Line: +# Authors: Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>
>, <Line: +#          Olivier Grisel <olivier.grisel@ensta.org>
>, <Line: +#
>, <Line: +# License: BSD 3 clause
>, <Line: +import matplotlib.pyplot as plt
>, <Line: +import numpy as np
>, <Line: +import gc
>, <Line: +import time
>, <Line: +from sklearn.externals.joblib import Memory
>, <Line: +from sklearn.linear_model import (LogisticRegression, SGDClassifier)
>, <Line: +from sklearn.datasets import fetch_rcv1
>, <Line: +from sklearn.linear_model.sag import get_auto_step_size
>, <Line: +from sklearn.linear_model.sag_fast import get_max_squared_sum
>, <Line: +try:
>, <Line: +    import lightning.classification as lightning_clf
>, <Line: +except ImportError:
>, <Line: +    lightning_clf = None
>, <Line: +m = Memory(cachedir='.', verbose=0)
>, <Line: +# compute logistic loss
>, <Line: +def get_loss(w, intercept, myX, myy, C):
>, <Line: +    n_samples = myX.shape[0]
>, <Line: +    w = w.ravel()
>, <Line: +    p = np.mean(np.log(1. + np.exp(-myy * (myX.dot(w) + intercept))))
>, <Line: +    print("%f + %f" % (p, w.dot(w) / 2. / C / n_samples))
>, <Line: +    p += w.dot(w) / 2. / C / n_samples
>, <Line: +    return p
>, <Line: +# We use joblib to cache individual fits. Note that we do not pass the dataset
>, <Line: +# as argument as the hashing would be too slow, so we assume that the dataset
>, <Line: +# never changes.
>, <Line: +@m.cache()
>, <Line: +def bench_one(name, clf_type, clf_params, n_iter):
>, <Line: +    clf = clf_type(**clf_params)
>, <Line: +    try:
>, <Line: +        clf.set_params(max_iter=n_iter, random_state=42)
>, <Line: +    except:
>, <Line: +        clf.set_params(n_iter=n_iter, random_state=42)
>, <Line: +    st = time.time()
>, <Line: +    clf.fit(X, y)
>, <Line: +    end = time.time()
>, <Line: +    try:
>, <Line: +        C = 1.0 / clf.alpha / n_samples
>, <Line: +    except:
>, <Line: +        C = clf.C
>, <Line: +    try:
>, <Line: +        intercept = clf.intercept_
>, <Line: +    except:
>, <Line: +        intercept = 0.
>, <Line: +    train_loss = get_loss(clf.coef_, intercept, X, y, C)
>, <Line: +    train_score = clf.score(X, y)
>, <Line: +    test_score = clf.score(X_test, y_test)
>, <Line: +    duration = end - st
>, <Line: +    return train_loss, train_score, test_score, duration
>, <Line: +def bench(clfs):
>, <Line: +    for (name, clf, iter_range, train_losses, train_scores,
>, <Line: +         test_scores, durations) in clfs:
>, <Line: +        print("training %s" % name)
>, <Line: +        clf_type = type(clf)
>, <Line: +        clf_params = clf.get_params()
>, <Line: +        for n_iter in iter_range:
>, <Line: +            gc.collect()
>, <Line: +            train_loss, train_score, test_score, duration = bench_one(
>, <Line: +                name, clf_type, clf_params, n_iter)
>, <Line: +            train_losses.append(train_loss)
>, <Line: +            train_scores.append(train_score)
>, <Line: +            test_scores.append(test_score)
>, <Line: +            durations.append(duration)
>, <Line: +            print("classifier: %s" % name)
>, <Line: +            print("train_loss: %.8f" % train_loss)
>, <Line: +            print("train_score: %.8f" % train_score)
>, <Line: +            print("test_score: %.8f" % test_score)
>, <Line: +            print("time for fit: %.8f seconds" % duration)
>, <Line: +            print("")
>, <Line: +        print("")
>, <Line: +    return clfs
>, <Line: +def plot_train_losses(clfs):
>, <Line: +    plt.figure()
>, <Line: +    for (name, _, _, train_losses, _, _, durations) in clfs:
>, <Line: +        plt.plot(durations, train_losses, '-o', label=name)
>, <Line: +        plt.legend(loc=0)
>, <Line: +        plt.xlabel("seconds")
>, <Line: +        plt.ylabel("train loss")
>, <Line: +def plot_train_scores(clfs):
>, <Line: +    plt.figure()
>, <Line: +    for (name, _, _, _, train_scores, _, durations) in clfs:
>, <Line: +        plt.plot(durations, train_scores, '-o', label=name)
>, <Line: +        plt.legend(loc=0)
>, <Line: +        plt.xlabel("seconds")
>, <Line: +        plt.ylabel("train score")
>, <Line: +        plt.ylim((0.92, 0.96))
>, <Line: +def plot_test_scores(clfs):
>, <Line: +    plt.figure()
>, <Line: +    for (name, _, _, _, _, test_scores, durations) in clfs:
>, <Line: +        plt.plot(durations, test_scores, '-o', label=name)
>, <Line: +        plt.legend(loc=0)
>, <Line: +        plt.xlabel("seconds")
>, <Line: +        plt.ylabel("test score")
>, <Line: +        plt.ylim((0.92, 0.96))
>, <Line: +def plot_dloss(clfs):
>, <Line: +    plt.figure()
>, <Line: +    pobj_final = []
>, <Line: +    for (name, _, _, train_losses, _, _, durations) in clfs:
>, <Line: +        pobj_final.append(train_losses[-1])
>, <Line: +    indices = np.argsort(pobj_final)
>, <Line: +    pobj_best = pobj_final[indices[0]]
>, <Line: +    for (name, _, _, train_losses, _, _, durations) in clfs:
>, <Line: +        log_pobj = np.log(abs(np.array(train_losses) - pobj_best)) / np.log(10)
>, <Line: +        plt.plot(durations, log_pobj, '-o', label=name)
>, <Line: +        plt.legend(loc=0)
>, <Line: +        plt.xlabel("seconds")
>, <Line: +        plt.ylabel("log(best - train_loss)")
>, <Line: +rcv1 = fetch_rcv1()
>, <Line: +X = rcv1.data
>, <Line: +n_samples, n_features = X.shape
>, <Line: +# consider the binary classification problem 'CCAT' vs the rest
>, <Line: +ccat_idx = rcv1.target_names.tolist().index('CCAT')
>, <Line: +y = rcv1.target.tocsc()[:, ccat_idx].toarray().ravel().astype(np.float64)
>, <Line: +y[y == 0] = -1
>, <Line: +# parameters
>, <Line: +C = 1.
>, <Line: +fit_intercept = True
>, <Line: +tol = 1.0e-14
>, <Line: +# max_iter range
>, <Line: +sgd_iter_range = list(range(1, 121, 10))
>, <Line: +newton_iter_range = list(range(1, 25, 3))
>, <Line: +lbfgs_iter_range = list(range(1, 242, 12))
>, <Line: +liblinear_iter_range = list(range(1, 37, 3))
>, <Line: +liblinear_dual_iter_range = list(range(1, 85, 6))
>, <Line: +sag_iter_range = list(range(1, 37, 3))
>, <Line: +clfs = [
>, <Line: +    ("LR-liblinear",
>, <Line: +     LogisticRegression(C=C, tol=tol,
>, <Line: +                        solver="liblinear", fit_intercept=fit_intercept,
>, <Line: +                        intercept_scaling=1),
>, <Line: +     liblinear_iter_range, [], [], [], []),
>, <Line: +    ("LR-liblinear-dual",
>, <Line: +     LogisticRegression(C=C, tol=tol, dual=True,
>, <Line: +                        solver="liblinear", fit_intercept=fit_intercept,
>, <Line: +                        intercept_scaling=1),
>, <Line: +     liblinear_dual_iter_range, [], [], [], []),
>, <Line: +    ("LR-SAG",
>, <Line: +     LogisticRegression(C=C, tol=tol,
>, <Line: +                        solver="sag", fit_intercept=fit_intercept),
>, <Line: +     sag_iter_range, [], [], [], []),
>, <Line: +    ("LR-newton-cg",
>, <Line: +     LogisticRegression(C=C, tol=tol, solver="newton-cg",
>, <Line: +                        fit_intercept=fit_intercept),
>, <Line: +     newton_iter_range, [], [], [], []),
>, <Line: +    ("LR-lbfgs",
>, <Line: +     LogisticRegression(C=C, tol=tol,
>, <Line: +                        solver="lbfgs", fit_intercept=fit_intercept),
>, <Line: +     lbfgs_iter_range, [], [], [], []),
>, <Line: +    ("SGD",
>, <Line: +     SGDClassifier(alpha=1.0 / C / n_samples, penalty='l2', loss='log',
>, <Line: +                   fit_intercept=fit_intercept, verbose=0),
>, <Line: +     sgd_iter_range, [], [], [], [])]
>, <Line: +if lightning_clf is not None and not fit_intercept:
>, <Line: +    alpha = 1. / C / n_samples
>, <Line: +    # compute the same step_size than in LR-sag
>, <Line: +    max_squared_sum = get_max_squared_sum(X)
>, <Line: +    step_size = get_auto_step_size(max_squared_sum, alpha, "log",
>, <Line: +                                   fit_intercept)
>, <Line: +    clfs.append(
>, <Line: +        ("Lightning-SVRG",
>, <Line: +         lightning_clf.SVRGClassifier(alpha=alpha, eta=step_size,
>, <Line: +                                      tol=tol, loss="log"),
>, <Line: +         sag_iter_range, [], [], [], []))
>, <Line: +    clfs.append(
>, <Line: +        ("Lightning-SAG",
>, <Line: +         lightning_clf.SAGClassifier(alpha=alpha, eta=step_size,
>, <Line: +                                     tol=tol, loss="log"),
>, <Line: +         sag_iter_range, [], [], [], []))
>, <Line: +    # We keep only 200 features, to have a dense dataset,
>, <Line: +    # and compare to lightning SAG, which seems incorrect in the sparse case.
>, <Line: +    X_csc = X.tocsc()
>, <Line: +    nnz_in_each_features = X_csc.indptr[1:] - X_csc.indptr[:-1]
>, <Line: +    X = X_csc[:, np.argsort(nnz_in_each_features)[-200:]]
>, <Line: +    X = X.toarray()
>, <Line: +    print("dataset: %.3f MB" % (X.nbytes / 1e6))
>, <Line: +# Split training and testing. Switch train and test subset compared to
>, <Line: +# LYRL2004 split, to have a larger training dataset.
>, <Line: +n = 23149
>, <Line: +X_test = X[:n, :]
>, <Line: +y_test = y[:n]
>, <Line: +X = X[n:, :]
>, <Line: +y = y[n:]
>, <Line: +clfs = bench(clfs)
>, <Line: +plot_train_scores(clfs)
>, <Line: +plot_test_scores(clfs)
>, <Line: +plot_train_losses(clfs)
>, <Line: +plot_dloss(clfs)
>, <Line: +plt.show()
>]
[]