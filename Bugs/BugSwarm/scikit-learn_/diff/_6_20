[<Line: +import scipy
>, <Line: +sp_version = tuple([int(s) for s in scipy.__version__.split('.')])
>, <Line: +                                   multi_class='multinomial'),
>, <Line: +                LogisticRegression(C=len(iris.data), solver='sag', tol=1e-2,
>, <Line: +                                   multi_class='ovr', random_state=42)]:
>, <Line: +        msg = ("Logistic Regression supports only liblinear, newton-cg, lbfgs"
>, <Line: +               " and sag solvers, got wrong_name")
>, <Line: +        for solver in ['liblinear', 'sag']:
>, <Line: +        for solver in ['newton-cg', 'lbfgs', 'sag']:
>, <Line: +    for solver in ('lbfgs', 'newton-cg', 'liblinear', 'sag'):
>, <Line: +        coefs, Cs, _ = f(logistic_regression_path)(
>, <Line: +            X, y, Cs=Cs, fit_intercept=False, tol=1e-5, solver=solver,
>, <Line: +            random_state=0)
>, <Line: +            lr = LogisticRegression(C=C, fit_intercept=False, tol=1e-5,
>, <Line: +                                    random_state=0)
>, <Line: +            assert_array_almost_equal(lr_coef, coefs[i], decimal=4,
>, <Line: +                                      err_msg="with solver = %s" % solver)
>, <Line: +    for solver in ('lbfgs', 'newton-cg', 'liblinear', 'sag'):
>, <Line: +        coefs, Cs, _ = f(logistic_regression_path)(
>, <Line: +            X, y, Cs=Cs, fit_intercept=True, tol=1e-6, solver=solver,
>, <Line: +            intercept_scaling=10000., random_state=0)
>, <Line: +                                intercept_scaling=10000., random_state=0)
>, <Line: +        assert_array_almost_equal(lr_coef, coefs[0], decimal=4,
>, <Line: +                                  err_msg="with solver = %s" % solver)
>, <Line: +    ncg = LogisticRegression(solver='newton-cg', fit_intercept=False)
>, <Line: +    lbf = LogisticRegression(solver='lbfgs', fit_intercept=False)
>, <Line: +    lib = LogisticRegression(fit_intercept=False)
>, <Line: +    sag = LogisticRegression(solver='sag', fit_intercept=False,
>, <Line: +                             random_state=42)
>, <Line: +    ncg.fit(X, y)
>, <Line: +    lbf.fit(X, y)
>, <Line: +    sag.fit(X, y)
>, <Line: +    lib.fit(X, y)
>, <Line: +    assert_array_almost_equal(ncg.coef_, lib.coef_, decimal=3)
>, <Line: +    assert_array_almost_equal(lib.coef_, lbf.coef_, decimal=3)
>, <Line: +    assert_array_almost_equal(ncg.coef_, lbf.coef_, decimal=3)
>, <Line: +    assert_array_almost_equal(sag.coef_, lib.coef_, decimal=3)
>, <Line: +    assert_array_almost_equal(sag.coef_, ncg.coef_, decimal=3)
>, <Line: +    assert_array_almost_equal(sag.coef_, lbf.coef_, decimal=3)
>, <Line: +    tol = 1e-6
>, <Line: +    ncg = LogisticRegression(solver='newton-cg', fit_intercept=False, tol=tol)
>, <Line: +    lbf = LogisticRegression(solver='lbfgs', fit_intercept=False, tol=tol)
>, <Line: +    lib = LogisticRegression(fit_intercept=False, tol=tol)
>, <Line: +    sag = LogisticRegression(solver='sag', fit_intercept=False, tol=tol,
>, <Line: +                             max_iter=1000, random_state=42)
>, <Line: +    ncg.fit(X, y)
>, <Line: +    lbf.fit(X, y)
>, <Line: +    sag.fit(X, y)
>, <Line: +    lib.fit(X, y)
>, <Line: +    assert_array_almost_equal(ncg.coef_, lib.coef_, decimal=4)
>, <Line: +    assert_array_almost_equal(lib.coef_, lbf.coef_, decimal=4)
>, <Line: +    assert_array_almost_equal(ncg.coef_, lbf.coef_, decimal=4)
>, <Line: +    assert_array_almost_equal(sag.coef_, lib.coef_, decimal=4)
>, <Line: +    assert_array_almost_equal(sag.coef_, ncg.coef_, decimal=4)
>, <Line: +    assert_array_almost_equal(sag.coef_, lbf.coef_, decimal=4)
>, <Line: +    clf_sag = LogisticRegressionCV(solver='sag', fit_intercept=False,
>, <Line: +                                   class_weight='balanced', max_iter=2000)
>, <Line: +    clf_sag.fit(X, y)
>, <Line: +    assert_array_almost_equal(clf_sag.coef_, clf_lbf.coef_, decimal=4)
>, <Line: +    assert_array_almost_equal(clf_lib.coef_, clf_sag.coef_, decimal=4)
>, <Line: +@ignore_warnings
>, <Line: +def test_max_iter():
>, <Line: +    # Test that the maximum number of iteration is reached
>, <Line: +    X, y_bin = iris.data, iris.target.copy()
>, <Line: +    y_bin[y_bin == 2] = 0
>, <Line: +    solvers = ['newton-cg', 'liblinear', 'sag']
>, <Line: +    # old scipy doesn't have maxiter
>, <Line: +    if sp_version >= (0, 12):
>, <Line: +        solvers.append('lbfgs')
>, <Line: +    for max_iter in range(1, 5):
>, <Line: +        for solver in solvers:
>, <Line: +            lr = LogisticRegression(max_iter=max_iter, tol=1e-15,
>, <Line: +                                    random_state=0, solver=solver)
>, <Line: +            lr.fit(X, y_bin)
>, <Line: +            assert_equal(lr.n_iter_[0], max_iter)
>, <Line: +def test_n_iter():
>, <Line: +    # Test that self.n_iter_ has the correct format.
>, <Line: +    X, y = iris.data, iris.target
>, <Line: +    y_bin = y.copy()
>, <Line: +    y_bin[y_bin == 2] = 0
>, <Line: +    n_Cs = 4
>, <Line: +    n_cv_fold = 2
>, <Line: +    for solver in ['newton-cg', 'liblinear', 'sag', 'lbfgs']:
>, <Line: +        # OvR case
>, <Line: +        n_classes = 1 if solver == 'liblinear' else np.unique(y).shape[0]
>, <Line: +        clf = LogisticRegression(tol=1e-2, multi_class='ovr',
>, <Line: +                                 solver=solver, C=1.,
>, <Line: +                                 random_state=42, max_iter=100)
>, <Line: +        clf.fit(X, y)
>, <Line: +        assert_equal(clf.n_iter_.shape, (n_classes,))
>, <Line: +        n_classes = np.unique(y).shape[0]
>, <Line: +        clf = LogisticRegressionCV(tol=1e-2, multi_class='ovr',
>, <Line: +                                   solver=solver, Cs=n_Cs, cv=n_cv_fold,
>, <Line: +                                   random_state=42, max_iter=100)
>, <Line: +        clf.fit(X, y)
>, <Line: +        assert_equal(clf.n_iter_.shape, (n_classes, n_cv_fold, n_Cs))
>, <Line: +        clf.fit(X, y_bin)
>, <Line: +        assert_equal(clf.n_iter_.shape, (1, n_cv_fold, n_Cs))
>, <Line: +        # multinomial case
>, <Line: +        n_classes = 1
>, <Line: +        if solver in ('liblinear', 'sag'):
>, <Line: +            break
>, <Line: +        clf = LogisticRegression(tol=1e-2, multi_class='multinomial',
>, <Line: +                                 solver=solver, C=1.,
>, <Line: +                                 random_state=42, max_iter=100)
>, <Line: +        clf.fit(X, y)
>, <Line: +        assert_equal(clf.n_iter_.shape, (n_classes,))
>, <Line: +        clf = LogisticRegressionCV(tol=1e-2, multi_class='multinomial',
>, <Line: +                                   solver=solver, Cs=n_Cs, cv=n_cv_fold,
>, <Line: +                                   random_state=42, max_iter=100)
>, <Line: +        clf.fit(X, y)
>, <Line: +        assert_equal(clf.n_iter_.shape, (n_classes, n_cv_fold, n_Cs))
>, <Line: +        clf.fit(X, y_bin)
>, <Line: +        assert_equal(clf.n_iter_.shape, (1, n_cv_fold, n_Cs))
>, <Line: +@ignore_warnings
>, <Line: +def test_warm_start():
>, <Line: +    # A 1-iteration second fit on same data should give almost same result
>, <Line: +    # with warm starting, and quite different result without warm starting.
>, <Line: +    # Warm starting does not work with liblinear solver.
>, <Line: +    X, y = iris.data, iris.target
>, <Line: +    solvers = ['newton-cg', 'sag']
>, <Line: +    # old scipy doesn't have maxiter
>, <Line: +    if sp_version >= (0, 12):
>, <Line: +        solvers.append('lbfgs')
>, <Line: +    for warm_start in [True, False]:
>, <Line: +        for fit_intercept in [True, False]:
>, <Line: +            for solver in solvers:
>, <Line: +                for multi_class in ['ovr', 'multinomial']:
>, <Line: +                    if solver == 'sag' and multi_class == 'multinomial':
>, <Line: +                        break
>, <Line: +                    clf = LogisticRegression(tol=1e-4, multi_class=multi_class,
>, <Line: +                                             warm_start=warm_start,
>, <Line: +                                             solver=solver,
>, <Line: +                                             random_state=42, max_iter=100,
>, <Line: +                                             fit_intercept=fit_intercept)
>, <Line: +                    clf.fit(X, y)
>, <Line: +                    coef_1 = clf.coef_
>, <Line: +                    clf.max_iter = 1
>, <Line: +                    with ignore_warnings():
>, <Line: +                        clf.fit(X, y)
>, <Line: +                    cum_diff = np.sum(np.abs(coef_1 - clf.coef_))
>, <Line: +                    msg = ("Warm starting issue with %s solver in %s mode "
>, <Line: +                           "with fit_intercept=%s and warm_start=%s"
>, <Line: +                           % (solver, multi_class, str(fit_intercept),
>, <Line: +                              str(warm_start)))
>, <Line: +                    if warm_start:
>, <Line: +                        assert_greater(2.0, cum_diff, msg)
>, <Line: +                    else:
>, <Line: +                        assert_greater(cum_diff, 2.0, msg)
>]
[<Line: -                                   multi_class='multinomial')]:
>, <Line: -        msg = ("Logistic Regression supports only liblinear, newton-cg and"
>, <Line: -               " lbfgs solvers, got wrong_name")
>, <Line: -        for solver in ['liblinear']:
>, <Line: -        for solver in ['newton-cg', 'lbfgs']:
>, <Line: -    for method in ('lbfgs', 'newton-cg', 'liblinear'):
>, <Line: -        coefs, Cs = f(logistic_regression_path)(
>, <Line: -            X, y, Cs=Cs, fit_intercept=False, tol=1e-16, solver=method)
>, <Line: -            lr = LogisticRegression(C=C, fit_intercept=False, tol=1e-16)
>, <Line: -            assert_array_almost_equal(lr_coef, coefs[i], decimal=4)
>, <Line: -    for method in ('lbfgs', 'newton-cg', 'liblinear'):
>, <Line: -        coefs, Cs = f(logistic_regression_path)(
>, <Line: -            X, y, Cs=Cs, fit_intercept=True, tol=1e-4, solver=method)
>, <Line: -                                intercept_scaling=10000)
>, <Line: -        assert_array_almost_equal(lr_coef, coefs[0], decimal=4)
>, <Line: -    clf_n = LogisticRegression(solver='newton-cg', fit_intercept=False)
>, <Line: -    clf_n.fit(X, y)
>, <Line: -    clf_lbf = LogisticRegression(solver='lbfgs', fit_intercept=False)
>, <Line: -    clf_lbf.fit(X, y)
>, <Line: -    clf_lib = LogisticRegression(fit_intercept=False)
>, <Line: -    clf_lib.fit(X, y)
>, <Line: -    assert_array_almost_equal(clf_n.coef_, clf_lib.coef_, decimal=3)
>, <Line: -    assert_array_almost_equal(clf_lib.coef_, clf_lbf.coef_, decimal=3)
>, <Line: -    assert_array_almost_equal(clf_n.coef_, clf_lbf.coef_, decimal=3)
>, <Line: -    clf_n = LogisticRegression(solver='newton-cg', fit_intercept=False)
>, <Line: -    clf_n.fit(X, y)
>, <Line: -    clf_lbf = LogisticRegression(solver='lbfgs', fit_intercept=False)
>, <Line: -    clf_lbf.fit(X, y)
>, <Line: -    clf_lib = LogisticRegression(fit_intercept=False)
>, <Line: -    clf_lib.fit(X, y)
>, <Line: -    assert_array_almost_equal(clf_n.coef_, clf_lib.coef_, decimal=4)
>, <Line: -    assert_array_almost_equal(clf_lib.coef_, clf_lbf.coef_, decimal=4)
>, <Line: -    assert_array_almost_equal(clf_n.coef_, clf_lbf.coef_, decimal=4)
>]