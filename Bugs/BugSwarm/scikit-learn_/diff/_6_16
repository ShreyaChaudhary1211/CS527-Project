[<Line: +# cython: boundscheck=False
>, <Line: +# cython: wraparound=False
>, <Line: +#
>, <Line: +# Authors: Danny Sullivan <dbsullivan23@gmail.com>
>, <Line: +#          Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>
>, <Line: +#
>, <Line: +# Licence: BSD 3 clause
>, <Line: +import scipy.sparse as sp
>, <Line: +from libc.math cimport fabs
>, <Line: +from ..utils.seq_dataset cimport SequentialDataset
>, <Line: +from .sgd_fast cimport LossFunction
>, <Line: +cdef inline double fmax(double x, double y) nogil:
>, <Line: +    if x > y:
>, <Line: +        return x
>, <Line: +    return y
>, <Line: +def sag(SequentialDataset dataset,
>, <Line: +        np.ndarray[double, ndim=1, mode='c'] weights_array,
>, <Line: +        double intercept,
>, <Line: +        int n_samples,
>, <Line: +        int n_features,
>, <Line: +        double tol,
>, <Line: +        int max_iter,
>, <Line: +        LossFunction loss,
>, <Line: +        double step_size,
>, <Line: +        double alpha,
>, <Line: +        np.ndarray[double, ndim=1, mode='c'] sum_gradient_init,
>, <Line: +        np.ndarray[double, ndim=1, mode='c'] gradient_memory_init,
>, <Line: +        np.ndarray[bint, ndim=1, mode='c'] seen_init,
>, <Line: +        int num_seen,
>, <Line: +        bint fit_intercept,
>, <Line: +        double intercept_sum_gradient,
>, <Line: +        double intercept_decay,
>, <Line: +        bint verbose):
>, <Line: +    # the prediction for y
>, <Line: +    cdef double p
>, <Line: +    # the number of pass through all samples
>, <Line: +    cdef int n_iter = 0
>, <Line: +    # precomputation since the step size does not change in this implementation
>, <Line: +    cdef double wscale_update = 1.0 - step_size * alpha
>, <Line: +        np.empty(n_samples, dtype=np.double, order="c")
>, <Line: +        np.zeros(n_features, dtype=np.int32, order="c")
>, <Line: +        np.zeros(n_features, dtype=np.double, order="c")
>, <Line: +    # helpers
>, <Line: +    cdef double val, cum_sum, gradient_change
>, <Line: +    cdef int j
>, <Line: +                        cum_sum = cumulative_sums[itr - 1]
>, <Line: +                        if feature_hist[idx] != 0:
>, <Line: +                            cum_sum -= cumulative_sums[feature_hist[idx] - 1]
>, <Line: +                        weights[idx] -= cum_sum * sum_gradient[idx]
>, <Line: +                # check to see that the intercept is not inf or NaN
>, <Line: +                p = (wscale * sparse_dot(x_data_ptr, x_ind_ptr, xnnz,
>, <Line: +                                         weights)) + intercept
>, <Line: +                gradient = loss._dloss(p, y) * sample_weight
>, <Line: +                gradient_change = gradient - gradient_memory[current_index]
>, <Line: +                    sum_gradient[idx] += gradient_change * val
>, <Line: +                    intercept_sum_gradient += gradient_change
>, <Line: +                    intercept -= (step_size * intercept_sum_gradient
>, <Line: +                                  / num_seen * intercept_decay)
>, <Line: +                # L2 regularization by simply rescaling the weights
>, <Line: +                wscale *= wscale_update
>, <Line: +                    cumulative_sums[0] = step_size / (wscale * num_seen)
>, <Line: +                                            + step_size / (wscale * num_seen))
>, <Line: +            if infinity:
>, <Line: +                break
>, <Line: +            n_iter += 1
>, <Line: +                              (n_iter, end_time - start_time))
>, <Line: +            if n_iter >= max_iter:
>, <Line: +                          " #%d. Lowering the step_size or scaling the input"
>, <Line: +                          " data with StandardScaler or"
>, <Line: +                          " MinMaxScaler might help.") % (n_iter + 1))
>, <Line: +    return intercept, num_seen, n_iter, intercept_sum_gradient
>, <Line: +                        int n_samples, int itr, double* cumulative_sums,
>, <Line: +    cdef int j
>, <Line: +    cdef double cum_sum
>, <Line: +        cum_sum = cumulative_sums[itr]
>, <Line: +        if feature_hist[j] != 0:
>, <Line: +            cum_sum -= cumulative_sums[feature_hist[j] - 1]
>, <Line: +        weights[j] -= cum_sum * sum_gradient[j]
>, <Line: +        feature_hist[j] = (itr + 1) % n_samples
>, <Line: +    cumulative_sums[itr % n_samples] = 0.0
>, <Line: +cdef double sparse_dot(double* x_data_ptr, int* x_ind_ptr, int xnnz,
>, <Line: +                       double* w_data_ptr) nogil:
>, <Line: +    """Compute dot product between sparse vector x and dense vector w."""
>, <Line: +    cdef int j, idx
>, <Line: +    cdef double innerprod = 0.0
>, <Line: +    # only consider nonzero values of x
>, <Line: +    for j in range(xnnz):
>, <Line: +        idx = x_ind_ptr[j]
>, <Line: +        innerprod += w_data_ptr[idx] * x_data_ptr[j]
>, <Line: +    return innerprod
>, <Line: +def get_max_squared_sum(X):
>, <Line: +    """Maximum squared sum of X over samples. 
>, <Line: +    Used in ``get_auto_step_size()``, for SAG solver.
>, <Line: +    Parameter
>, <Line: +    ---------
>, <Line: +    X : {numpy array, scipy CSR sparse matrix}, shape (n_samples, n_features)
>, <Line: +        Training vector. X must be in C order.
>, <Line: +    Returns
>, <Line: +    -------
>, <Line: +    max_squared_sum : double
>, <Line: +        Maximum squared sum of X over samples.
>, <Line: +    """
>, <Line: +    # CSR sparse matrix X
>, <Line: +    cdef np.ndarray[double] X_data
>, <Line: +    cdef np.ndarray[int] X_indptr
>, <Line: +    cdef double *X_data_ptr
>, <Line: +    cdef int *X_indptr_ptr
>, <Line: +    cdef int offset
>, <Line: +    # Dense numpy array X
>, <Line: +    cdef np.ndarray[double, ndim=2] X_ndarray
>, <Line: +    cdef int stride
>, <Line: +    # Both cases
>, <Line: +    cdef bint sparse = sp.issparse(X)
>, <Line: +    cdef int n_samples = X.shape[0]
>, <Line: +    cdef int nnz = X.shape[1]
>, <Line: +    cdef int i, j
>, <Line: +    cdef double val
>, <Line: +    if sparse:
>, <Line: +        X_data = X.data
>, <Line: +        X_indptr = X.indptr
>, <Line: +        X_data_ptr = <double *>X_data.data
>, <Line: +        X_indptr_ptr = <int *>X_indptr.data
>, <Line: +    else:
>, <Line: +        X_ndarray = X
>, <Line: +        stride = X_ndarray.strides[0] / X_ndarray.itemsize
>, <Line: +        x_data_ptr = <double *>X_ndarray.data - stride
>, <Line: +            # find next sample data
>, <Line: +            if sparse:
>, <Line: +                offset = X_indptr_ptr[i]
>, <Line: +                nnz = X_indptr_ptr[i + 1] - offset
>, <Line: +                x_data_ptr = X_data_ptr + offset
>, <Line: +            else:
>, <Line: +                x_data_ptr += stride
>, <Line: +            # sum of squared non-zero features
>, <Line: +            for j in range(nnz):
>, <Line: +    if not skl_isfinite(max_squared_sum):
>, <Line: +        raise ValueError("Floating-point under-/overflow occurred")
>, <Line: +    return max_squared_sum
>]
[<Line: -import warnings
>, <Line: -from libc.math cimport fmax, fabs, exp, log
>, <Line: -from sklearn.utils.seq_dataset cimport SequentialDataset
>, <Line: -cdef class LossFunction:
>, <Line: -    """Base class for convex loss functions"""
>, <Line: -    cdef double loss(self, double p, double y) nogil:
>, <Line: -        """Evaluate the loss function.
>, <Line: -        Parameters
>, <Line: -        ----------
>, <Line: -        p : double
>, <Line: -            The prediction, p = w^T x
>, <Line: -        y : double
>, <Line: -            The true value (aka target)
>, <Line: -        Returns
>, <Line: -        -------
>, <Line: -        double
>, <Line: -            The loss evaluated at `p` and `y`.
>, <Line: -        """
>, <Line: -        pass
>, <Line: -    def dloss(self, double p, double y):
>, <Line: -        """Evaluate the derivative of the loss function with respect to
>, <Line: -        the prediction `p`.
>, <Line: -        Parameters
>, <Line: -        ----------
>, <Line: -        p : double
>, <Line: -            The prediction, p = w^T x
>, <Line: -        y : double
>, <Line: -            The true value (aka target)
>, <Line: -        Returns
>, <Line: -        -------
>, <Line: -        double
>, <Line: -            The derivative of the loss function with regards to `p`.
>, <Line: -        """
>, <Line: -        return self._dloss(p, y)
>, <Line: -    cdef double _dloss(self, double p, double y) nogil:
>, <Line: -        pass
>, <Line: -cdef class Regression(LossFunction):
>, <Line: -    """Base class for loss functions for regression"""
>, <Line: -    cdef double loss(self, double p, double y) nogil:
>, <Line: -        pass
>, <Line: -    cdef double _dloss(self, double p, double y) nogil:
>, <Line: -        pass
>, <Line: -cdef class Classification(LossFunction):
>, <Line: -    """Base class for loss functions for classification"""
>, <Line: -    cdef double loss(self, double p, double y) nogil:
>, <Line: -        return 0.
>, <Line: -    cdef double _dloss(self, double p, double y) nogil:
>, <Line: -        return 0.
>, <Line: -cdef class Log(Classification):
>, <Line: -    """Logistic regression loss for binary classification with y in {-1, 1}"""
>, <Line: -    cdef double loss(self, double p, double y) nogil:
>, <Line: -        cdef double z = p * y
>, <Line: -        # approximately equal and saves the computation of the log
>, <Line: -        if z > 18:
>, <Line: -            return exp(-z)
>, <Line: -        if z < -18:
>, <Line: -            return -z
>, <Line: -        return log(1.0 + exp(-z))
>, <Line: -    cdef double _dloss(self, double p, double y) nogil:
>, <Line: -        cdef double z = p * y
>, <Line: -        # approximately equal and saves the computation of the log
>, <Line: -        if z > 18.0:
>, <Line: -            return exp(-z) * -y
>, <Line: -        if z < -18.0:
>, <Line: -            return -y
>, <Line: -        return -y / (exp(z) + 1.0)
>, <Line: -    def __reduce__(self):
>, <Line: -        return Log, ()
>, <Line: -cdef class SquaredLoss(Regression):
>, <Line: -    """Squared loss traditional used in linear regression."""
>, <Line: -    cdef double loss(self, double p, double y) nogil:
>, <Line: -        return 0.5 * (p - y) * (p - y)
>, <Line: -    cdef double _dloss(self, double p, double y) nogil:
>, <Line: -        return p - y
>, <Line: -    def __reduce__(self):
>, <Line: -        return SquaredLoss, ()
>, <Line: -def sag_sparse(SequentialDataset dataset,
>, <Line: -               np.ndarray[double, ndim=1, mode='c'] weights_array,
>, <Line: -               double intercept,
>, <Line: -               int n_samples,
>, <Line: -               int n_features,
>, <Line: -               double tol,
>, <Line: -               int max_iter,
>, <Line: -               LossFunction loss,
>, <Line: -               double eta,
>, <Line: -               double alpha,
>, <Line: -               np.ndarray[double, ndim=1, mode='c'] sum_gradient_init,
>, <Line: -               np.ndarray[double, ndim=1, mode='c'] gradient_memory_init,
>, <Line: -               np.ndarray[bint, ndim=1, mode='c'] seen_init,
>, <Line: -               int num_seen,
>, <Line: -               double weight_pos,
>, <Line: -               double weight_neg,
>, <Line: -               bint fit_intercept,
>, <Line: -               double intercept_sum_gradient,
>, <Line: -               double intercept_decay,
>, <Line: -               bint verbose):
>, <Line: -    # the total number of interations through the data
>, <Line: -    cdef int total_iter = 0
>, <Line: -    # helper variable for the weight of a pos/neg class
>, <Line: -    cdef double class_weight
>, <Line: -    # whether or not the max iter has been reached
>, <Line: -    cdef bint max_iter_reached = False
>, <Line: -        np.empty(n_samples,
>, <Line: -                 dtype=np.double,
>, <Line: -                 order="c")
>, <Line: -        np.zeros(n_features,
>, <Line: -                 dtype=np.int32,
>, <Line: -                 order="c")
>, <Line: -        np.zeros(n_features,
>, <Line: -                 dtype=np.double,
>, <Line: -                 order="c")
>, <Line: -                # dataset.next(&x_data_ptr,
>, <Line: -                #              &x_ind_ptr,
>, <Line: -                #              &xnnz,
>, <Line: -                #              &y,
>, <Line: -                #              &sample_weight)
>, <Line: -                # current_index = itr
>, <Line: -                        if feature_hist[idx] == 0:
>, <Line: -                            weights[idx] -= (cumulative_sums[itr - 1] *
>, <Line: -                                             sum_gradient[idx])
>, <Line: -                        else:
>, <Line: -                            weights[idx] -= \
>, <Line: -                                ((cumulative_sums[itr - 1] -
>, <Line: -                                  cumulative_sums[feature_hist[idx] - 1]) *
>, <Line: -                                 sum_gradient[idx])
>, <Line: -                # check to see if we have already encountered a bad weight or
>, <Line: -                # that the intercept is not inf or NaN
>, <Line: -                p = (wscale * dot(x_data_ptr, x_ind_ptr,
>, <Line: -                     weights, xnnz)) + intercept
>, <Line: -                gradient = loss._dloss(p, y)
>, <Line: -                # find the class_weight
>, <Line: -                if y > 0.0:
>, <Line: -                    class_weight = weight_pos
>, <Line: -                else:
>, <Line: -                    class_weight = weight_neg
>, <Line: -                gradient *= sample_weight * class_weight
>, <Line: -                    update = val * gradient
>, <Line: -                    sum_gradient[idx] += (update -
>, <Line: -                                          gradient_memory[current_index] * val)
>, <Line: -                    intercept_sum_gradient += \
>, <Line: -                        gradient - gradient_memory[current_index]
>, <Line: -                    intercept -= (eta *
>, <Line: -                                  (intercept_sum_gradient / num_seen) *
>, <Line: -                                  intercept_decay)
>, <Line: -                wscale *= 1.0 - eta * alpha
>, <Line: -                    cumulative_sums[0] = eta / (wscale * num_seen)
>, <Line: -                                            + eta / (wscale * num_seen))
>, <Line: -                total_iter += 1
>, <Line: -                              ((total_iter / n_samples),
>, <Line: -                              (end_time - start_time)))
>, <Line: -            if total_iter / n_samples >= max_iter:
>, <Line: -                max_iter_reached = True
>, <Line: -                          " #%d. Lowering the eta0 or scaling the input data"
>, <Line: -                          " with StandardScaler or"
>, <Line: -                          " MinMaxScaler might help.") % (total_iter + 1))
>, <Line: -    return intercept, num_seen, max_iter_reached, intercept_sum_gradient
>, <Line: -                        int n_samples, int total_iter, double* cumulative_sums,
>, <Line: -        if feature_hist[j] == 0:
>, <Line: -            weights[j] -= (cumulative_sums[total_iter] *
>, <Line: -                           sum_gradient[j])
>, <Line: -        else:
>, <Line: -            weights[j] -= ((cumulative_sums[total_iter] -
>, <Line: -                            cumulative_sums[feature_hist[j] - 1]) *
>, <Line: -                            sum_gradient[j])
>, <Line: -        feature_hist[j] = (total_iter + 1) % n_samples
>, <Line: -    cumulative_sums[total_iter % n_samples] = 0.0
>, <Line: -def get_auto_eta(SequentialDataset dataset, double alpha,
>, <Line: -                 int n_samples, LossFunction loss, bint fit_intercept):
>, <Line: -    cdef int *x_ind_ptr
>, <Line: -    cdef double y
>, <Line: -    cdef double sample_weight
>, <Line: -    cdef int xnnz
>, <Line: -            dataset.next(&x_data_ptr,
>, <Line: -                         &x_ind_ptr,
>, <Line: -                         &xnnz,
>, <Line: -                         &y,
>, <Line: -                         &sample_weight)
>, <Line: -            for j in range(xnnz):
>, <Line: -    if isinstance(loss, Classification):
>, <Line: -        # Lipschitz for log loss
>, <Line: -        return 4.0 / (max_squared_sum + fit_intercept + 4.0 * alpha)
>, <Line: -    else:
>, <Line: -        # Lipschitz for squared loss
>, <Line: -        return 1.0 / (max_squared_sum + fit_intercept + alpha)
>, <Line: -cdef double dot(double* x_data_ptr, int* x_ind_ptr, double* w_data_ptr,
>, <Line: -                int xnnz) nogil:
>, <Line: -        cdef int j
>, <Line: -        cdef int idx
>, <Line: -        cdef double innerprod = 0.0
>, <Line: -        for j in range(xnnz):
>, <Line: -            idx = x_ind_ptr[j]
>, <Line: -            innerprod += w_data_ptr[idx] * x_data_ptr[j]
>, <Line: -        return innerprod
>]